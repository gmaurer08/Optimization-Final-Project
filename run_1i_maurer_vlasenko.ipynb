{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbUOx2UYNtf1"
      },
      "source": [
        "# Optimization Methods For Data Science\n",
        "## Final Project - Part 2: SVM\n",
        "\n",
        "Géraldine V. Maurer, Viktoriia Vlasenko"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from functions_1j_maurer_vlasenko import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvPqrN1ht4EO"
      },
      "source": [
        "### Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "yZX0PhP1t3jH",
        "outputId": "f0b31afb-a1df-4766-e019-0cfeaf5701c3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feat_1</th>\n",
              "      <th>feat_2</th>\n",
              "      <th>feat_3</th>\n",
              "      <th>feat_4</th>\n",
              "      <th>feat_5</th>\n",
              "      <th>feat_6</th>\n",
              "      <th>feat_7</th>\n",
              "      <th>feat_8</th>\n",
              "      <th>feat_9</th>\n",
              "      <th>feat_10</th>\n",
              "      <th>...</th>\n",
              "      <th>feat_24</th>\n",
              "      <th>feat_25</th>\n",
              "      <th>feat_26</th>\n",
              "      <th>feat_27</th>\n",
              "      <th>feat_28</th>\n",
              "      <th>feat_29</th>\n",
              "      <th>feat_30</th>\n",
              "      <th>feat_31</th>\n",
              "      <th>feat_32</th>\n",
              "      <th>gt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.686191</td>\n",
              "      <td>-0.989465</td>\n",
              "      <td>-0.920503</td>\n",
              "      <td>1.607427</td>\n",
              "      <td>-0.896248</td>\n",
              "      <td>1.118974</td>\n",
              "      <td>-0.969456</td>\n",
              "      <td>1.811707</td>\n",
              "      <td>2.560955</td>\n",
              "      <td>3.803463</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.862891</td>\n",
              "      <td>-0.909545</td>\n",
              "      <td>-0.915361</td>\n",
              "      <td>-0.952061</td>\n",
              "      <td>-0.989461</td>\n",
              "      <td>1.911855</td>\n",
              "      <td>1.409705</td>\n",
              "      <td>2.303997</td>\n",
              "      <td>-0.981840</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.887917</td>\n",
              "      <td>4.915272</td>\n",
              "      <td>-0.939446</td>\n",
              "      <td>-0.343677</td>\n",
              "      <td>-0.964685</td>\n",
              "      <td>-0.478649</td>\n",
              "      <td>4.342395</td>\n",
              "      <td>-0.332870</td>\n",
              "      <td>-0.768041</td>\n",
              "      <td>-0.815375</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.939201</td>\n",
              "      <td>-0.965917</td>\n",
              "      <td>-0.969461</td>\n",
              "      <td>-0.934799</td>\n",
              "      <td>5.304822</td>\n",
              "      <td>0.934790</td>\n",
              "      <td>-0.410701</td>\n",
              "      <td>0.284690</td>\n",
              "      <td>4.919212</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.923215</td>\n",
              "      <td>2.746968</td>\n",
              "      <td>-0.918085</td>\n",
              "      <td>0.047804</td>\n",
              "      <td>-0.908587</td>\n",
              "      <td>-0.451752</td>\n",
              "      <td>2.984481</td>\n",
              "      <td>0.535007</td>\n",
              "      <td>-0.591029</td>\n",
              "      <td>-0.324043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.809726</td>\n",
              "      <td>-0.929934</td>\n",
              "      <td>-0.891814</td>\n",
              "      <td>-0.881796</td>\n",
              "      <td>3.415373</td>\n",
              "      <td>1.044108</td>\n",
              "      <td>-0.442615</td>\n",
              "      <td>0.033648</td>\n",
              "      <td>2.628199</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.268866</td>\n",
              "      <td>-0.408416</td>\n",
              "      <td>-0.935145</td>\n",
              "      <td>0.731800</td>\n",
              "      <td>-0.922438</td>\n",
              "      <td>0.221781</td>\n",
              "      <td>-0.046606</td>\n",
              "      <td>1.149634</td>\n",
              "      <td>0.592136</td>\n",
              "      <td>1.357959</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.834968</td>\n",
              "      <td>-0.937475</td>\n",
              "      <td>-0.917737</td>\n",
              "      <td>-0.929519</td>\n",
              "      <td>-0.226282</td>\n",
              "      <td>1.608048</td>\n",
              "      <td>0.276169</td>\n",
              "      <td>1.246468</td>\n",
              "      <td>-0.363367</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.529231</td>\n",
              "      <td>-0.829957</td>\n",
              "      <td>-0.897425</td>\n",
              "      <td>0.921280</td>\n",
              "      <td>-0.865304</td>\n",
              "      <td>0.331018</td>\n",
              "      <td>-0.644940</td>\n",
              "      <td>1.296097</td>\n",
              "      <td>1.166863</td>\n",
              "      <td>2.036034</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.775411</td>\n",
              "      <td>-0.881967</td>\n",
              "      <td>-0.864018</td>\n",
              "      <td>-0.908001</td>\n",
              "      <td>-0.784495</td>\n",
              "      <td>1.329586</td>\n",
              "      <td>0.547925</td>\n",
              "      <td>1.195395</td>\n",
              "      <td>-0.810089</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
              "0  2.686191 -0.989465 -0.920503  1.607427 -0.896248  1.118974 -0.969456   \n",
              "1 -0.887917  4.915272 -0.939446 -0.343677 -0.964685 -0.478649  4.342395   \n",
              "2 -0.923215  2.746968 -0.918085  0.047804 -0.908587 -0.451752  2.984481   \n",
              "3 -0.268866 -0.408416 -0.935145  0.731800 -0.922438  0.221781 -0.046606   \n",
              "4  0.529231 -0.829957 -0.897425  0.921280 -0.865304  0.331018 -0.644940   \n",
              "\n",
              "     feat_8    feat_9   feat_10  ...   feat_24   feat_25   feat_26   feat_27  \\\n",
              "0  1.811707  2.560955  3.803463  ... -0.862891 -0.909545 -0.915361 -0.952061   \n",
              "1 -0.332870 -0.768041 -0.815375  ... -0.939201 -0.965917 -0.969461 -0.934799   \n",
              "2  0.535007 -0.591029 -0.324043  ... -0.809726 -0.929934 -0.891814 -0.881796   \n",
              "3  1.149634  0.592136  1.357959  ... -0.834968 -0.937475 -0.917737 -0.929519   \n",
              "4  1.296097  1.166863  2.036034  ... -0.775411 -0.881967 -0.864018 -0.908001   \n",
              "\n",
              "    feat_28   feat_29   feat_30   feat_31   feat_32  gt  \n",
              "0 -0.989461  1.911855  1.409705  2.303997 -0.981840  54  \n",
              "1  5.304822  0.934790 -0.410701  0.284690  4.919212  18  \n",
              "2  3.415373  1.044108 -0.442615  0.033648  2.628199  26  \n",
              "3 -0.226282  1.608048  0.276169  1.246468 -0.363367  33  \n",
              "4 -0.784495  1.329586  0.547925  1.195395 -0.810089  35  \n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"https://raw.githubusercontent.com/gmaurer08/Optimization-Final-Project/refs/heads/main/AGE_PREDICTION.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDmGTiXxuBsj"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-lH4rJruU2L",
        "outputId": "12f6dc76-8366-464a-bf30-06df76d89ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features: 32 columns\n",
            "Data shape: (20475, 32)\n",
            "Target range: 10.00-89.00\n",
            "Train set: (16380, 32)\n",
            "Test set: (4095, 32)\n"
          ]
        }
      ],
      "source": [
        "# Separate features and target\n",
        "feature_cols = [col for col in data.columns if col.startswith('feat')]\n",
        "X = data[feature_cols].values\n",
        "y = data['gt'].values\n",
        "\n",
        "print(f\"Features: {len(feature_cols)} columns\")\n",
        "print(f\"Data shape: {X.shape}\")\n",
        "print(f\"Target range: {y.min():.2f}-{y.max():.2f}\")\n",
        "\n",
        "# Split data into train/test sets\n",
        "n_total = len(X)\n",
        "n_train = int(0.8*n_total)    # 80% for training (used with CV inside)\n",
        "# Remaining 20% for testing\n",
        "\n",
        "# Shuffle indices\n",
        "indices = np.random.permutation(n_total)\n",
        "train_idx = indices[:n_train]\n",
        "test_idx = indices[n_train:]\n",
        "\n",
        "X_train, y_train = X[train_idx], y[train_idx]\n",
        "X_test, y_test = X[test_idx], y[test_idx]\n",
        "\n",
        "# Normalize features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Train set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd8YES8Sunh_"
      },
      "source": [
        "### Find the best hyperparameters with Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhoxaNNHum51",
        "outputId": "72dbf347-c18b-4579-9c33-2987f2f56f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting hyperparameter search\n",
            "\n",
            "[1/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1662.687561\n",
            "Final loss: 86.543487\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 83.8020, Val Loss: 104.0971\n",
            "Train MAPE: 21.4188%, Val MAPE: 24.0400%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1656.307293\n",
            "Final loss: 85.941984\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 83.5605, Val Loss: 104.5303\n",
            "Train MAPE: 21.4481%, Val MAPE: 24.0472%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1655.990383\n",
            "Final loss: 85.076143\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 82.1874, Val Loss: 104.1293\n",
            "Train MAPE: 21.4136%, Val MAPE: 24.2008%\n",
            "Results: Val Loss = 104.2522, Val MAPE = 24.0960%\n",
            "\n",
            "[2/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1663.482074\n",
            "Final loss: 94.523133\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.5920, Val Loss: 96.2830\n",
            "Train MAPE: 22.6040%, Val MAPE: 23.2168%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1657.101807\n",
            "Final loss: 93.848498\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.5394, Val Loss: 97.0862\n",
            "Train MAPE: 22.4940%, Val MAPE: 23.3064%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1656.784896\n",
            "Final loss: 93.876194\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.3343, Val Loss: 96.5319\n",
            "Train MAPE: 22.4960%, Val MAPE: 23.5129%\n",
            "Results: Val Loss = 96.6337, Val MAPE = 23.3454%\n",
            "\n",
            "[3/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1671.427207\n",
            "Final loss: 100.213998\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 96.3871, Val Loss: 95.9783\n",
            "Train MAPE: 23.5810%, Val MAPE: 23.3848%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1665.046939\n",
            "Final loss: 99.655560\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.7649, Val Loss: 97.1591\n",
            "Train MAPE: 23.4338%, Val MAPE: 23.4934%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1664.730028\n",
            "Final loss: 99.737972\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 96.1692, Val Loss: 96.6093\n",
            "Train MAPE: 23.4270%, Val MAPE: 23.7299%\n",
            "Results: Val Loss = 96.5822, Val MAPE = 23.5360%\n",
            "\n",
            "[4/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1705.824063\n",
            "Final loss: 93.067217\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.9668, Val Loss: 97.1133\n",
            "Train MAPE: 22.4811%, Val MAPE: 23.2932%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1699.620812\n",
            "Final loss: 92.468634\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.3987, Val Loss: 97.9803\n",
            "Train MAPE: 22.4359%, Val MAPE: 23.4183%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1699.011322\n",
            "Final loss: 92.695631\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.2242, Val Loss: 97.9817\n",
            "Train MAPE: 22.4864%, Val MAPE: 23.6660%\n",
            "Results: Val Loss = 97.6918, Val MAPE = 23.4592%\n",
            "\n",
            "[5/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1706.618577\n",
            "Final loss: 97.971879\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.5352, Val Loss: 95.7129\n",
            "Train MAPE: 23.4251%, Val MAPE: 23.3156%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1700.415325\n",
            "Final loss: 97.389419\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.0993, Val Loss: 96.7141\n",
            "Train MAPE: 23.2898%, Val MAPE: 23.4146%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1699.805835\n",
            "Final loss: 97.731633\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.5808, Val Loss: 96.1333\n",
            "Train MAPE: 23.3276%, Val MAPE: 23.6351%\n",
            "Results: Val Loss = 96.1868, Val MAPE = 23.4551%\n",
            "\n",
            "[6/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1714.563709\n",
            "Final loss: 109.938841\n",
            "Optimization successful: True\n",
            "Number of iterations: 454\n",
            "Train Loss: 99.1693, Val Loss: 98.4892\n",
            "Train MAPE: 24.0243%, Val MAPE: 23.8202%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1708.360458\n",
            "Final loss: 109.431473\n",
            "Optimization successful: True\n",
            "Number of iterations: 579\n",
            "Train Loss: 98.3601, Val Loss: 99.1786\n",
            "Train MAPE: 23.8551%, Val MAPE: 23.8407%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1707.750967\n"
          ]
        }
      ],
      "source": [
        "# Find best hyperparameters using cross-validation\n",
        "best_parameters, search_results = hyperparameter_search(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-8c4C5duvAB"
      },
      "source": [
        "### Training the final model with the best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJsFXLw-u9rx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1675.734443\n",
            "Final loss: 98.689045\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "1. Number of layers L chosen: 4\n",
            "2. Number of neurons N chosen: 64\n",
            "3. Value of lambda chosen: 0.01\n",
            "4. Other hyperparameters: activation = sigmoid, maxiter = 1000\n",
            "5. Optimization solver: L-BFGS-B\n",
            "6. Number of iterations: 1000\n",
            "7. Optimization time: 242.45 seconds\n",
            "8. Training Error (without regularization): 95.003767\n",
            "9. Test Error: 91.734070\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "np.random.seed(42)\n",
        "\n",
        "# Train final model with best parameters\n",
        "start_time = time.time()\n",
        "final_weights, final_biases, optimization_result, initial_loss, final_loss = train_network(\n",
        "    X_train, y_train,\n",
        "    layer_sizes=best_parameters['layers'],\n",
        "    activation=best_parameters['activation'],\n",
        "    lambda_reg=best_parameters['lambda_reg'],\n",
        "    method='L-BFGS-B',\n",
        "    maxiter=1000,\n",
        "    seed=42\n",
        ")\n",
        "# Compute time difference from start\n",
        "end_time = time.time()\n",
        "optimization_time = end_time - start_time\n",
        "\n",
        "# Make predictions on all datasets\n",
        "y_train_pred = predict(X_train, final_weights, final_biases, best_parameters['activation'])\n",
        "y_test_pred = predict(X_test, final_weights, final_biases, best_parameters['activation'])\n",
        "\n",
        "# Compute Mean Squared Error\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "# Compute Mean Absolute Percentage Error\n",
        "train_mape = MAPE(y_train, y_train_pred)\n",
        "test_mape = MAPE(y_test, y_test_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"1. Number of layers L chosen: {len(best_parameters['layers'])-1}\")\n",
        "print(f\"2. Number of neurons N chosen: {max(best_parameters['layers'][1:-1]) if len(best_parameters['layers'])>2 else best_parameters['layers'][1]}\")\n",
        "print(f\"3. Value of lambda chosen: {best_parameters['lambda_reg']}\")\n",
        "print(f\"4. Other hyperparameters: activation = {best_parameters['activation']}, maxiter = 1000\")\n",
        "print(f\"5. Optimization solver: L-BFGS-B\")\n",
        "print(f\"6. Number of iterations: {optimization_result.nit}\")\n",
        "print(f\"7. Optimization time: {optimization_time:.2f} seconds\")\n",
        "print(f\"8. Training Error (without regularization): {train_mse:.6f}\")\n",
        "print(f\"9. Test Error: {test_mse:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI8IfwTUw3SI"
      },
      "source": [
        "### Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_C8DE9BtrxZ"
      },
      "outputs": [],
      "source": [
        "# Save detailed results\n",
        "results_summary = {\n",
        "    'best_config': best_parameters,\n",
        "    'optimization_result': {\n",
        "        'success': optimization_result.success,\n",
        "        'message': optimization_result.message,\n",
        "        'iterations': optimization_result.nit,\n",
        "        'optimization_time': optimization_time,\n",
        "        'initial_loss': initial_loss,\n",
        "        'final_loss': final_loss\n",
        "    },\n",
        "    'performance_metrics': {\n",
        "        'train_mse': train_mse,\n",
        "        'test_mse': test_mse,\n",
        "        'train_mape': train_mape,\n",
        "        'test_mape': test_mape\n",
        "    },\n",
        "    'hyperparameter_search_results': search_results\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"results_summary.pkl\", \"wb\") as f:\n",
        "    pickle.dump(results_summary,f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'best_config': {'layers': [32, 64, 32, 16, 1],\n",
              "  'activation': 'sigmoid',\n",
              "  'lambda_reg': 0.01},\n",
              " 'optimization_result': {'success': False,\n",
              "  'message': 'STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT',\n",
              "  'iterations': 1000,\n",
              "  'optimization_time': 271.80328464508057,\n",
              "  'initial_loss': np.float64(1734.996120744687),\n",
              "  'final_loss': np.float64(98.68810268057803)},\n",
              " 'performance_metrics': {'train_mse': 95.00780876569202,\n",
              "  'test_mse': 91.72251600512398,\n",
              "  'train_mape': np.float64(23.31217961550376),\n",
              "  'test_mape': np.float64(22.763067005720114)},\n",
              " 'hyperparameter_search_results': [{'layers': [32, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(105.28236443534229),\n",
              "   'avg_val_mape': np.float64(24.15918092646488),\n",
              "   'avg_train_loss': np.float64(83.06689650781153),\n",
              "   'avg_train_mape': np.float64(21.368623353417735)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(96.57989694851581),\n",
              "   'avg_val_mape': np.float64(23.33201616016622),\n",
              "   'avg_train_loss': np.float64(90.74128528608217),\n",
              "   'avg_train_mape': np.float64(22.50038821875548)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(96.58168979669226),\n",
              "   'avg_val_mape': np.float64(23.533563156889347),\n",
              "   'avg_train_loss': np.float64(96.1063984376835),\n",
              "   'avg_train_mape': np.float64(23.477681609121948)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(97.38572320007745),\n",
              "   'avg_val_mape': np.float64(23.45958053242103),\n",
              "   'avg_train_loss': np.float64(90.6414750373043),\n",
              "   'avg_train_mape': np.float64(22.5210897778736)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(96.1843494077687),\n",
              "   'avg_val_mape': np.float64(23.4524690560708),\n",
              "   'avg_train_loss': np.float64(95.38371065398205),\n",
              "   'avg_train_mape': np.float64(23.34285539577247)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(98.92126178941129),\n",
              "   'avg_val_mape': np.float64(23.932410492009257),\n",
              "   'avg_train_loss': np.float64(98.73512858360311),\n",
              "   'avg_train_mape': np.float64(23.908984126136172)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(106.51779187794801),\n",
              "   'avg_val_mape': np.float64(24.339220692569214),\n",
              "   'avg_train_loss': np.float64(80.15701376218526),\n",
              "   'avg_train_mape': np.float64(21.01499127303705)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(96.33316305106173),\n",
              "   'avg_val_mape': np.float64(23.287229768754944),\n",
              "   'avg_train_loss': np.float64(91.00663977662225),\n",
              "   'avg_train_mape': np.float64(22.542755218719225)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(96.53600575058981),\n",
              "   'avg_val_mape': np.float64(23.509141889937798),\n",
              "   'avg_train_loss': np.float64(96.07995643462631),\n",
              "   'avg_train_mape': np.float64(23.455354614217715)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(97.53837173472267),\n",
              "   'avg_val_mape': np.float64(23.453389105952578),\n",
              "   'avg_train_loss': np.float64(91.0520957078284),\n",
              "   'avg_train_mape': np.float64(22.576282087931265)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(96.245066062104),\n",
              "   'avg_val_mape': np.float64(23.452182378086068),\n",
              "   'avg_train_loss': np.float64(95.51230883514854),\n",
              "   'avg_train_mape': np.float64(23.345366073351986)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(98.28308862448652),\n",
              "   'avg_val_mape': np.float64(23.836367601883165),\n",
              "   'avg_train_loss': np.float64(98.13929218882923),\n",
              "   'avg_train_mape': np.float64(23.81737693711503)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(99.88581065623845),\n",
              "   'avg_val_mape': np.float64(23.684823314452604),\n",
              "   'avg_train_loss': np.float64(88.99235548867625),\n",
              "   'avg_train_mape': np.float64(22.299907950542817)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(97.9802640698869),\n",
              "   'avg_val_mape': np.float64(23.438514278710922),\n",
              "   'avg_train_loss': np.float64(90.71464792550609),\n",
              "   'avg_train_mape': np.float64(22.528983317601206)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(96.82016172616244),\n",
              "   'avg_val_mape': np.float64(23.605911620149588),\n",
              "   'avg_train_loss': np.float64(96.05586905341642),\n",
              "   'avg_train_mape': np.float64(23.52689851052132)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(98.26061803195962),\n",
              "   'avg_val_mape': np.float64(23.486416898489036),\n",
              "   'avg_train_loss': np.float64(91.1521467852981),\n",
              "   'avg_train_mape': np.float64(22.62867995865354)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(95.80049262143079),\n",
              "   'avg_val_mape': np.float64(23.412311798280673),\n",
              "   'avg_train_loss': np.float64(94.6657768043825),\n",
              "   'avg_train_mape': np.float64(23.270015585685982)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(100.7476180379781),\n",
              "   'avg_val_mape': np.float64(24.210947663133197),\n",
              "   'avg_train_loss': np.float64(100.49697653574572),\n",
              "   'avg_train_mape': np.float64(24.181985146027824)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(104.58120277864703),\n",
              "   'avg_val_mape': np.float64(24.025344173900248),\n",
              "   'avg_train_loss': np.float64(82.68322342887348),\n",
              "   'avg_train_mape': np.float64(21.416769670674768)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(98.20394111813677),\n",
              "   'avg_val_mape': np.float64(23.434442010834683),\n",
              "   'avg_train_loss': np.float64(89.76853831904674),\n",
              "   'avg_train_mape': np.float64(22.38075227256826)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(96.21444648067124),\n",
              "   'avg_val_mape': np.float64(23.470996146800584),\n",
              "   'avg_train_loss': np.float64(95.37536618750691),\n",
              "   'avg_train_mape': np.float64(23.37393034615555)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(97.80930675525467),\n",
              "   'avg_val_mape': np.float64(23.437222606015695),\n",
              "   'avg_train_loss': np.float64(91.20579894304558),\n",
              "   'avg_train_mape': np.float64(22.629598016181912)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(95.92826975579833),\n",
              "   'avg_val_mape': np.float64(23.40580737049832),\n",
              "   'avg_train_loss': np.float64(94.7742109552101),\n",
              "   'avg_train_mape': np.float64(23.25352962762741)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(99.28225397452665),\n",
              "   'avg_val_mape': np.float64(23.98344539408994),\n",
              "   'avg_train_loss': np.float64(99.12037307499533),\n",
              "   'avg_train_mape': np.float64(23.961601614062587)}]}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_summary"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
