{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbUOx2UYNtf1"
      },
      "source": [
        "# Optimization Methods For Data Science\n",
        "## Final Project - Part 2: SVM\n",
        "\n",
        "Géraldine V. Maurer, Viktoriia Vlasenko"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from functions_1j_maurer_vlasenko import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvPqrN1ht4EO"
      },
      "source": [
        "### Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "yZX0PhP1t3jH",
        "outputId": "f0b31afb-a1df-4766-e019-0cfeaf5701c3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feat_1</th>\n",
              "      <th>feat_2</th>\n",
              "      <th>feat_3</th>\n",
              "      <th>feat_4</th>\n",
              "      <th>feat_5</th>\n",
              "      <th>feat_6</th>\n",
              "      <th>feat_7</th>\n",
              "      <th>feat_8</th>\n",
              "      <th>feat_9</th>\n",
              "      <th>feat_10</th>\n",
              "      <th>...</th>\n",
              "      <th>feat_24</th>\n",
              "      <th>feat_25</th>\n",
              "      <th>feat_26</th>\n",
              "      <th>feat_27</th>\n",
              "      <th>feat_28</th>\n",
              "      <th>feat_29</th>\n",
              "      <th>feat_30</th>\n",
              "      <th>feat_31</th>\n",
              "      <th>feat_32</th>\n",
              "      <th>gt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.686191</td>\n",
              "      <td>-0.989465</td>\n",
              "      <td>-0.920503</td>\n",
              "      <td>1.607427</td>\n",
              "      <td>-0.896248</td>\n",
              "      <td>1.118974</td>\n",
              "      <td>-0.969456</td>\n",
              "      <td>1.811707</td>\n",
              "      <td>2.560955</td>\n",
              "      <td>3.803463</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.862891</td>\n",
              "      <td>-0.909545</td>\n",
              "      <td>-0.915361</td>\n",
              "      <td>-0.952061</td>\n",
              "      <td>-0.989461</td>\n",
              "      <td>1.911855</td>\n",
              "      <td>1.409705</td>\n",
              "      <td>2.303997</td>\n",
              "      <td>-0.981840</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.887917</td>\n",
              "      <td>4.915272</td>\n",
              "      <td>-0.939446</td>\n",
              "      <td>-0.343677</td>\n",
              "      <td>-0.964685</td>\n",
              "      <td>-0.478649</td>\n",
              "      <td>4.342395</td>\n",
              "      <td>-0.332870</td>\n",
              "      <td>-0.768041</td>\n",
              "      <td>-0.815375</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.939201</td>\n",
              "      <td>-0.965917</td>\n",
              "      <td>-0.969461</td>\n",
              "      <td>-0.934799</td>\n",
              "      <td>5.304822</td>\n",
              "      <td>0.934790</td>\n",
              "      <td>-0.410701</td>\n",
              "      <td>0.284690</td>\n",
              "      <td>4.919212</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.923215</td>\n",
              "      <td>2.746968</td>\n",
              "      <td>-0.918085</td>\n",
              "      <td>0.047804</td>\n",
              "      <td>-0.908587</td>\n",
              "      <td>-0.451752</td>\n",
              "      <td>2.984481</td>\n",
              "      <td>0.535007</td>\n",
              "      <td>-0.591029</td>\n",
              "      <td>-0.324043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.809726</td>\n",
              "      <td>-0.929934</td>\n",
              "      <td>-0.891814</td>\n",
              "      <td>-0.881796</td>\n",
              "      <td>3.415373</td>\n",
              "      <td>1.044108</td>\n",
              "      <td>-0.442615</td>\n",
              "      <td>0.033648</td>\n",
              "      <td>2.628199</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.268866</td>\n",
              "      <td>-0.408416</td>\n",
              "      <td>-0.935145</td>\n",
              "      <td>0.731800</td>\n",
              "      <td>-0.922438</td>\n",
              "      <td>0.221781</td>\n",
              "      <td>-0.046606</td>\n",
              "      <td>1.149634</td>\n",
              "      <td>0.592136</td>\n",
              "      <td>1.357959</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.834968</td>\n",
              "      <td>-0.937475</td>\n",
              "      <td>-0.917737</td>\n",
              "      <td>-0.929519</td>\n",
              "      <td>-0.226282</td>\n",
              "      <td>1.608048</td>\n",
              "      <td>0.276169</td>\n",
              "      <td>1.246468</td>\n",
              "      <td>-0.363367</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.529231</td>\n",
              "      <td>-0.829957</td>\n",
              "      <td>-0.897425</td>\n",
              "      <td>0.921280</td>\n",
              "      <td>-0.865304</td>\n",
              "      <td>0.331018</td>\n",
              "      <td>-0.644940</td>\n",
              "      <td>1.296097</td>\n",
              "      <td>1.166863</td>\n",
              "      <td>2.036034</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.775411</td>\n",
              "      <td>-0.881967</td>\n",
              "      <td>-0.864018</td>\n",
              "      <td>-0.908001</td>\n",
              "      <td>-0.784495</td>\n",
              "      <td>1.329586</td>\n",
              "      <td>0.547925</td>\n",
              "      <td>1.195395</td>\n",
              "      <td>-0.810089</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
              "0  2.686191 -0.989465 -0.920503  1.607427 -0.896248  1.118974 -0.969456   \n",
              "1 -0.887917  4.915272 -0.939446 -0.343677 -0.964685 -0.478649  4.342395   \n",
              "2 -0.923215  2.746968 -0.918085  0.047804 -0.908587 -0.451752  2.984481   \n",
              "3 -0.268866 -0.408416 -0.935145  0.731800 -0.922438  0.221781 -0.046606   \n",
              "4  0.529231 -0.829957 -0.897425  0.921280 -0.865304  0.331018 -0.644940   \n",
              "\n",
              "     feat_8    feat_9   feat_10  ...   feat_24   feat_25   feat_26   feat_27  \\\n",
              "0  1.811707  2.560955  3.803463  ... -0.862891 -0.909545 -0.915361 -0.952061   \n",
              "1 -0.332870 -0.768041 -0.815375  ... -0.939201 -0.965917 -0.969461 -0.934799   \n",
              "2  0.535007 -0.591029 -0.324043  ... -0.809726 -0.929934 -0.891814 -0.881796   \n",
              "3  1.149634  0.592136  1.357959  ... -0.834968 -0.937475 -0.917737 -0.929519   \n",
              "4  1.296097  1.166863  2.036034  ... -0.775411 -0.881967 -0.864018 -0.908001   \n",
              "\n",
              "    feat_28   feat_29   feat_30   feat_31   feat_32  gt  \n",
              "0 -0.989461  1.911855  1.409705  2.303997 -0.981840  54  \n",
              "1  5.304822  0.934790 -0.410701  0.284690  4.919212  18  \n",
              "2  3.415373  1.044108 -0.442615  0.033648  2.628199  26  \n",
              "3 -0.226282  1.608048  0.276169  1.246468 -0.363367  33  \n",
              "4 -0.784495  1.329586  0.547925  1.195395 -0.810089  35  \n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"https://raw.githubusercontent.com/gmaurer08/Optimization-Final-Project/refs/heads/main/AGE_PREDICTION.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDmGTiXxuBsj"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-lH4rJruU2L",
        "outputId": "12f6dc76-8366-464a-bf30-06df76d89ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features: 32 columns\n",
            "Data shape: (20475, 32)\n",
            "Target range: 10.00-89.00\n",
            "Train set: (16380, 32)\n",
            "Test set: (4095, 32)\n"
          ]
        }
      ],
      "source": [
        "# Separate features and target\n",
        "feature_cols = [col for col in data.columns if col.startswith('feat')]\n",
        "X = data[feature_cols].values\n",
        "y = data['gt'].values\n",
        "\n",
        "print(f\"Features: {len(feature_cols)} columns\")\n",
        "print(f\"Data shape: {X.shape}\")\n",
        "print(f\"Target range: {y.min():.2f}-{y.max():.2f}\")\n",
        "\n",
        "# Split data into train/test sets\n",
        "n_total = len(X)\n",
        "n_train = int(0.8*n_total)    # 80% for training (used with CV inside)\n",
        "# Remaining 20% for testing\n",
        "\n",
        "# Shuffle indices\n",
        "indices = np.random.permutation(n_total)\n",
        "train_idx = indices[:n_train]\n",
        "test_idx = indices[n_train:]\n",
        "\n",
        "X_train, y_train = X[train_idx], y[train_idx]\n",
        "X_test, y_test = X[test_idx], y[test_idx]\n",
        "\n",
        "# Normalize features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Train set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd8YES8Sunh_"
      },
      "source": [
        "### Find the best hyperparameters with Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhoxaNNHum51",
        "outputId": "72dbf347-c18b-4579-9c33-2987f2f56f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting hyperparameter search\n",
            "\n",
            "[1/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1662.687561\n",
            "Final loss: 86.543487\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 83.8020, Val Loss: 104.0971\n",
            "Train MAPE: 21.4188%, Val MAPE: 24.0400%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1656.307293\n",
            "Final loss: 85.941984\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 83.5605, Val Loss: 104.5303\n",
            "Train MAPE: 21.4481%, Val MAPE: 24.0472%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1655.990383\n",
            "Final loss: 85.076143\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 82.1874, Val Loss: 104.1293\n",
            "Train MAPE: 21.4136%, Val MAPE: 24.2008%\n",
            "Results: Val Loss = 104.2522, Val MAPE = 24.0960%\n",
            "\n",
            "[2/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1663.482074\n",
            "Final loss: 94.523133\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.5920, Val Loss: 96.2830\n",
            "Train MAPE: 22.6040%, Val MAPE: 23.2168%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1657.101807\n",
            "Final loss: 93.848498\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.5394, Val Loss: 97.0862\n",
            "Train MAPE: 22.4940%, Val MAPE: 23.3064%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1656.784896\n",
            "Final loss: 93.876194\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.3343, Val Loss: 96.5319\n",
            "Train MAPE: 22.4960%, Val MAPE: 23.5129%\n",
            "Results: Val Loss = 96.6337, Val MAPE = 23.3454%\n",
            "\n",
            "[3/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1671.427207\n",
            "Final loss: 100.213998\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 96.3871, Val Loss: 95.9783\n",
            "Train MAPE: 23.5810%, Val MAPE: 23.3848%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1665.046939\n",
            "Final loss: 99.655560\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.7649, Val Loss: 97.1591\n",
            "Train MAPE: 23.4338%, Val MAPE: 23.4934%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1664.730028\n",
            "Final loss: 99.737972\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 96.1692, Val Loss: 96.6093\n",
            "Train MAPE: 23.4270%, Val MAPE: 23.7299%\n",
            "Results: Val Loss = 96.5822, Val MAPE = 23.5360%\n",
            "\n",
            "[4/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1705.824063\n",
            "Final loss: 93.067217\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.9668, Val Loss: 97.1133\n",
            "Train MAPE: 22.4811%, Val MAPE: 23.2932%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1699.620812\n",
            "Final loss: 92.468634\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.3987, Val Loss: 97.9803\n",
            "Train MAPE: 22.4359%, Val MAPE: 23.4183%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1699.011322\n",
            "Final loss: 92.695631\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.2242, Val Loss: 97.9817\n",
            "Train MAPE: 22.4864%, Val MAPE: 23.6660%\n",
            "Results: Val Loss = 97.6918, Val MAPE = 23.4592%\n",
            "\n",
            "[5/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1706.618577\n",
            "Final loss: 97.971879\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.5352, Val Loss: 95.7129\n",
            "Train MAPE: 23.4251%, Val MAPE: 23.3156%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1700.415325\n",
            "Final loss: 97.389419\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.0993, Val Loss: 96.7141\n",
            "Train MAPE: 23.2898%, Val MAPE: 23.4146%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1699.805835\n",
            "Final loss: 97.731633\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.5808, Val Loss: 96.1333\n",
            "Train MAPE: 23.3276%, Val MAPE: 23.6351%\n",
            "Results: Val Loss = 96.1868, Val MAPE = 23.4551%\n",
            "\n",
            "[6/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1714.563709\n",
            "Final loss: 109.938841\n",
            "Optimization successful: True\n",
            "Number of iterations: 454\n",
            "Train Loss: 99.1693, Val Loss: 98.4892\n",
            "Train MAPE: 24.0243%, Val MAPE: 23.8202%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1708.360458\n",
            "Final loss: 109.431473\n",
            "Optimization successful: True\n",
            "Number of iterations: 579\n",
            "Train Loss: 98.3601, Val Loss: 99.1786\n",
            "Train MAPE: 23.8551%, Val MAPE: 23.8407%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1707.750967\n",
            "Final loss: 109.583226\n",
            "Optimization successful: True\n",
            "Number of iterations: 430\n",
            "Train Loss: 98.7153, Val Loss: 99.1380\n",
            "Train MAPE: 23.8519%, Val MAPE: 24.1408%\n",
            "Results: Val Loss = 98.9353, Val MAPE = 23.9339%\n",
            "\n",
            "[7/24] Testing configuration:\n",
            "Layers: [128, 64] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1684.120771\n",
            "Final loss: 82.565978\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 78.8588, Val Loss: 110.3050\n",
            "Train MAPE: 20.8106%, Val MAPE: 24.6252%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1677.975138\n",
            "Final loss: 81.816726\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 77.8884, Val Loss: 111.5004\n",
            "Train MAPE: 20.5566%, Val MAPE: 24.6813%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1677.089649\n",
            "Final loss: 83.687473\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 79.9446, Val Loss: 105.0913\n",
            "Train MAPE: 20.9682%, Val MAPE: 24.3623%\n",
            "Results: Val Loss = 108.9655, Val MAPE = 24.5563%\n",
            "\n",
            "[8/24] Testing configuration:\n",
            "Layers: [128, 64] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1685.365483\n",
            "Final loss: 94.372960\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.6243, Val Loss: 96.4880\n",
            "Train MAPE: 22.6097%, Val MAPE: 23.2355%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1679.219850\n",
            "Final loss: 93.710150\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.7591, Val Loss: 96.8622\n",
            "Train MAPE: 22.4916%, Val MAPE: 23.2504%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1678.334361\n",
            "Final loss: 94.166740\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.2729, Val Loss: 96.0558\n",
            "Train MAPE: 22.6123%, Val MAPE: 23.4527%\n",
            "Results: Val Loss = 96.4687, Val MAPE = 23.3129%\n",
            "\n",
            "[9/24] Testing configuration:\n",
            "Layers: [128, 64] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1697.812608\n",
            "Final loss: 98.876497\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 96.4402, Val Loss: 95.9994\n",
            "Train MAPE: 23.5658%, Val MAPE: 23.3696%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1691.666975\n",
            "Final loss: 98.215877\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.6753, Val Loss: 97.0449\n",
            "Train MAPE: 23.3983%, Val MAPE: 23.4612%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1690.781487\n",
            "Final loss: 98.509538\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 96.1548, Val Loss: 96.6194\n",
            "Train MAPE: 23.4068%, Val MAPE: 23.7064%\n",
            "Results: Val Loss = 96.5546, Val MAPE = 23.5124%\n",
            "\n",
            "[10/24] Testing configuration:\n",
            "Layers: [128, 64] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1678.320815\n",
            "Final loss: 93.174921\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.4676, Val Loss: 96.8333\n",
            "Train MAPE: 22.6140%, Val MAPE: 23.3647%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1672.160199\n",
            "Final loss: 92.599696\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.5743, Val Loss: 97.8646\n",
            "Train MAPE: 22.4758%, Val MAPE: 23.4539%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1671.491303\n",
            "Final loss: 93.119882\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.9958, Val Loss: 96.3751\n",
            "Train MAPE: 22.6030%, Val MAPE: 23.5309%\n",
            "Results: Val Loss = 97.0243, Val MAPE = 23.4498%\n",
            "\n",
            "[11/24] Testing configuration:\n",
            "Layers: [128, 64] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1679.565528\n",
            "Final loss: 97.592135\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.5979, Val Loss: 95.7113\n",
            "Train MAPE: 23.4077%, Val MAPE: 23.2919%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1673.404911\n",
            "Final loss: 96.986847\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.1358, Val Loss: 96.6990\n",
            "Train MAPE: 23.2807%, Val MAPE: 23.3965%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1672.736015\n",
            "Final loss: 97.325944\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.6549, Val Loss: 96.2120\n",
            "Train MAPE: 23.3222%, Val MAPE: 23.6386%\n",
            "Results: Val Loss = 96.2074, Val MAPE = 23.4423%\n",
            "\n",
            "[12/24] Testing configuration:\n",
            "Layers: [128, 64] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1692.012653\n",
            "Final loss: 106.391865\n",
            "Optimization successful: True\n",
            "Number of iterations: 402\n",
            "Train Loss: 98.4845, Val Loss: 97.6406\n",
            "Train MAPE: 23.9180%, Val MAPE: 23.7081%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1685.852036\n",
            "Final loss: 105.833766\n",
            "Optimization successful: True\n",
            "Number of iterations: 408\n",
            "Train Loss: 97.8203, Val Loss: 98.6336\n",
            "Train MAPE: 23.7706%, Val MAPE: 23.7674%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1685.183140\n",
            "Final loss: 105.969615\n",
            "Optimization successful: True\n",
            "Number of iterations: 335\n",
            "Train Loss: 98.0363, Val Loss: 98.4965\n",
            "Train MAPE: 23.7529%, Val MAPE: 24.0221%\n",
            "Results: Val Loss = 98.2569, Val MAPE = 23.8325%\n",
            "\n",
            "[13/24] Testing configuration:\n",
            "Layers: [64, 32, 16] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1660.519337\n",
            "Final loss: 96.902649\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 94.6262, Val Loss: 96.9785\n",
            "Train MAPE: 23.2480%, Val MAPE: 23.4127%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1653.545473\n",
            "Final loss: 92.825937\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.2626, Val Loss: 98.9243\n",
            "Train MAPE: 22.6296%, Val MAPE: 23.5327%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1653.438587\n",
            "Final loss: 94.975065\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 93.3916, Val Loss: 98.4945\n",
            "Train MAPE: 23.1544%, Val MAPE: 23.8092%\n",
            "Results: Val Loss = 98.1324, Val MAPE = 23.5849%\n",
            "\n",
            "[14/24] Testing configuration:\n",
            "Layers: [64, 32, 16] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1661.497498\n",
            "Final loss: 94.717896\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.7980, Val Loss: 96.7063\n",
            "Train MAPE: 22.4760%, Val MAPE: 23.2723%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1654.523633\n",
            "Final loss: 93.816245\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.0444, Val Loss: 98.6514\n",
            "Train MAPE: 22.3863%, Val MAPE: 23.5723%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1654.416748\n",
            "Final loss: 93.647344\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 89.3697, Val Loss: 97.7415\n",
            "Train MAPE: 22.3812%, Val MAPE: 23.5942%\n",
            "Results: Val Loss = 97.6997, Val MAPE = 23.4796%\n",
            "\n",
            "[15/24] Testing configuration:\n",
            "Layers: [64, 32, 16] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1671.279104\n",
            "Final loss: 103.088268\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 96.0729, Val Loss: 96.4080\n",
            "Train MAPE: 23.5706%, Val MAPE: 23.4586%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1664.305240\n",
            "Final loss: 103.550423\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.9433, Val Loss: 97.6418\n",
            "Train MAPE: 23.5319%, Val MAPE: 23.6074%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1664.198354\n",
            "Final loss: 103.885662\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 96.4065, Val Loss: 96.9906\n",
            "Train MAPE: 23.5401%, Val MAPE: 23.8513%\n",
            "Results: Val Loss = 97.0135, Val MAPE = 23.6391%\n",
            "\n",
            "[16/24] Testing configuration:\n",
            "Layers: [64, 32, 16] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1679.072019\n",
            "Final loss: 93.220108\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.6408, Val Loss: 97.0853\n",
            "Train MAPE: 22.6176%, Val MAPE: 23.2657%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1672.917641\n",
            "Final loss: 92.409752\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.7107, Val Loss: 98.5061\n",
            "Train MAPE: 22.5554%, Val MAPE: 23.5818%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1672.279188\n",
            "Final loss: 92.791500\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.6445, Val Loss: 97.4333\n",
            "Train MAPE: 22.6607%, Val MAPE: 23.6375%\n",
            "Results: Val Loss = 97.6749, Val MAPE = 23.4950%\n",
            "\n",
            "[17/24] Testing configuration:\n",
            "Layers: [64, 32, 16] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1680.050180\n",
            "Final loss: 98.745873\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 94.8416, Val Loss: 95.5189\n",
            "Train MAPE: 23.3577%, Val MAPE: 23.2950%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1673.895801\n",
            "Final loss: 98.177207\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 94.3232, Val Loss: 96.3948\n",
            "Train MAPE: 23.2092%, Val MAPE: 23.3963%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1673.257349\n",
            "Final loss: 98.596844\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 94.8614, Val Loss: 95.6836\n",
            "Train MAPE: 23.2590%, Val MAPE: 23.5866%\n",
            "Results: Val Loss = 95.8658, Val MAPE = 23.4260%\n",
            "\n",
            "[18/24] Testing configuration:\n",
            "Layers: [64, 32, 16] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1689.831786\n",
            "Final loss: 118.337772\n",
            "Optimization successful: True\n",
            "Number of iterations: 417\n",
            "Train Loss: 100.9753, Val Loss: 100.5946\n",
            "Train MAPE: 24.2901%, Val MAPE: 24.1321%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1683.677408\n",
            "Final loss: 117.966742\n",
            "Optimization successful: True\n",
            "Number of iterations: 761\n",
            "Train Loss: 100.0946, Val Loss: 100.7410\n",
            "Train MAPE: 24.1312%, Val MAPE: 24.0778%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1683.038955\n",
            "Final loss: 118.043865\n",
            "Optimization successful: True\n",
            "Number of iterations: 902\n",
            "Train Loss: 100.4951, Val Loss: 100.9631\n",
            "Train MAPE: 24.1345%, Val MAPE: 24.4317%\n",
            "Results: Val Loss = 100.7662, Val MAPE = 24.2139%\n",
            "\n",
            "[19/24] Testing configuration:\n",
            "Layers: [128, 64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1668.523537\n",
            "Final loss: 87.077550\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 85.1769, Val Loss: 101.6130\n",
            "Train MAPE: 21.7058%, Val MAPE: 23.7288%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1662.918698\n",
            "Final loss: 85.593840\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 83.5150, Val Loss: 105.1354\n",
            "Train MAPE: 21.5471%, Val MAPE: 24.0271%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1661.901585\n",
            "Final loss: 88.100093\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 86.6014, Val Loss: 102.1999\n",
            "Train MAPE: 22.0514%, Val MAPE: 23.9262%\n",
            "Results: Val Loss = 102.9828, Val MAPE = 23.8940%\n",
            "\n",
            "[20/24] Testing configuration:\n",
            "Layers: [128, 64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1670.165269\n",
            "Final loss: 93.511484\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.1143, Val Loss: 97.7832\n",
            "Train MAPE: 22.3469%, Val MAPE: 23.4245%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1664.560431\n",
            "Final loss: 92.881614\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 89.3679, Val Loss: 99.4851\n",
            "Train MAPE: 22.3047%, Val MAPE: 23.4556%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1663.543318\n",
            "Final loss: 93.326531\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 89.6651, Val Loss: 97.4995\n",
            "Train MAPE: 22.4244%, Val MAPE: 23.5234%\n",
            "Results: Val Loss = 98.2559, Val MAPE = 23.4678%\n",
            "\n",
            "[21/24] Testing configuration:\n",
            "Layers: [128, 64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1686.582597\n",
            "Final loss: 99.684749\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.6196, Val Loss: 95.6793\n",
            "Train MAPE: 23.4631%, Val MAPE: 23.3326%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1680.977759\n",
            "Final loss: 99.094142\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.0924, Val Loss: 96.7716\n",
            "Train MAPE: 23.3426%, Val MAPE: 23.4651%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1679.960645\n",
            "Final loss: 99.438938\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.5629, Val Loss: 96.1387\n",
            "Train MAPE: 23.3643%, Val MAPE: 23.6632%\n",
            "Results: Val Loss = 96.1965, Val MAPE = 23.4870%\n",
            "\n",
            "[22/24] Testing configuration:\n",
            "Layers: [128, 64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1679.753534\n",
            "Final loss: 92.980304\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.3608, Val Loss: 96.6414\n",
            "Train MAPE: 22.6064%, Val MAPE: 23.2582%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1673.618527\n",
            "Final loss: 92.408813\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.7834, Val Loss: 97.6586\n",
            "Train MAPE: 22.5256%, Val MAPE: 23.3338%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1672.969700\n",
            "Final loss: 92.728954\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.9827, Val Loss: 97.0059\n",
            "Train MAPE: 22.6864%, Val MAPE: 23.5422%\n",
            "Results: Val Loss = 97.1020, Val MAPE = 23.3781%\n",
            "\n",
            "[23/24] Testing configuration:\n",
            "Layers: [128, 64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1681.395267\n",
            "Final loss: 97.919618\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 94.8754, Val Loss: 95.5700\n",
            "Train MAPE: 23.3191%, Val MAPE: 23.2690%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1675.260260\n",
            "Final loss: 97.414580\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 94.3052, Val Loss: 96.3747\n",
            "Train MAPE: 23.1692%, Val MAPE: 23.3666%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1674.611432\n",
            "Final loss: 97.831083\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.1122, Val Loss: 95.6891\n",
            "Train MAPE: 23.2599%, Val MAPE: 23.5702%\n",
            "Results: Val Loss = 95.8780, Val MAPE = 23.4019%\n",
            "\n",
            "[24/24] Testing configuration:\n",
            "Layers: [128, 64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1697.812594\n",
            "Final loss: 112.148557\n",
            "Optimization successful: True\n",
            "Number of iterations: 395\n",
            "Train Loss: 99.4664, Val Loss: 98.8650\n",
            "Train MAPE: 24.0596%, Val MAPE: 23.8694%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1691.677587\n",
            "Final loss: 111.716471\n",
            "Optimization successful: True\n",
            "Number of iterations: 401\n",
            "Train Loss: 98.8362, Val Loss: 99.5064\n",
            "Train MAPE: 23.9212%, Val MAPE: 23.8899%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1691.028760\n",
            "Final loss: 111.820673\n",
            "Optimization successful: True\n",
            "Number of iterations: 405\n",
            "Train Loss: 99.0595, Val Loss: 99.4759\n",
            "Train MAPE: 23.9047%, Val MAPE: 24.1918%\n",
            "Results: Val Loss = 99.2824, Val MAPE = 23.9837%\n",
            "\n",
            "\n",
            "HYPERPARAMETER SEARCH COMPLETE\n",
            "Best validation loss: 95.865761\n",
            "Best configuration:\n",
            "Hidden layers: [64, 32, 16]\n",
            "Activations: sigmoid\n",
            "Lambda: 0.01\n"
          ]
        }
      ],
      "source": [
        "# Find best hyperparameters using cross-validation\n",
        "best_parameters, search_results = hyperparameter_search(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-8c4C5duvAB"
      },
      "source": [
        "### Training the final model with the best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1675.734443\n",
            "Final loss: 98.689045\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "1. Number of layers L chosen: 4\n",
            "2. Number of neurons N chosen: 64\n",
            "3. Value of lambda chosen: 0.01\n",
            "4. Other hyperparameters: activation = sigmoid, maxiter = 1000\n",
            "5. Optimization solver: L-BFGS-B\n",
            "6. Number of iterations: 1000\n",
            "7. Optimization time: 308.11 seconds\n",
            "8. Training Error (MSE, without regularization): 95.003767\n",
            "9. Test Error (MSE): 91.734070\n",
            "10. Initial Training MAPE: 99.2923%\n",
            "11. Initial Test MAPE: 99.2925%\n",
            "12. Final Training MAPE: 23.3173%\n",
            "13. Final Test MAPE: 22.7634%\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "np.random.seed(42)\n",
        "\n",
        "# Train final model with best parameters\n",
        "start_time = time.time()\n",
        "final_weights, final_biases, optimization_result, initial_loss, final_loss = train_network(\n",
        "    X_train, y_train,\n",
        "    layer_sizes=best_parameters['layers'],\n",
        "    activation=best_parameters['activation'],\n",
        "    lambda_reg=best_parameters['lambda_reg'],\n",
        "    method='L-BFGS-B',\n",
        "    maxiter=1000,\n",
        "    seed=42\n",
        ")\n",
        "# Compute time difference from start\n",
        "end_time = time.time()\n",
        "optimization_time = end_time - start_time\n",
        "\n",
        "# For initial loss, we need to re-initialize weights in same way\n",
        "init_weights, init_biases = initialize_network(best_parameters['layers'])\n",
        "y_train_pred_init = predict(X_train, init_weights, init_biases, best_parameters['activation'])\n",
        "y_test_pred_init = predict(X_test, init_weights, init_biases, best_parameters['activation'])\n",
        "\n",
        "# Final predictions (after training)\n",
        "y_train_pred_final = predict(X_train, final_weights, final_biases, best_parameters['activation'])\n",
        "y_test_pred_final = predict(X_test, final_weights, final_biases, best_parameters['activation'])\n",
        "\n",
        "# Compute Mean Squared Error\n",
        "train_mse = mean_squared_error(y_train, y_train_pred_final)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred_final)\n",
        "\n",
        "# Compute Mean Absolute Percentage Error\n",
        "train_mape_init = MAPE(y_train, y_train_pred_init)\n",
        "test_mape_init = MAPE(y_test, y_test_pred_init)\n",
        "train_mape_final = MAPE(y_train, y_train_pred_final)\n",
        "test_mape_final = MAPE(y_test, y_test_pred_final)\n",
        "\n",
        "# Output results\n",
        "print(f\"1. Number of layers L chosen: {len(best_parameters['layers'])-1}\")\n",
        "print(f\"2. Number of neurons N chosen: {max(best_parameters['layers'][1:-1]) if len(best_parameters['layers'])>2 else best_parameters['layers'][1]}\")\n",
        "print(f\"3. Value of lambda chosen: {best_parameters['lambda_reg']}\")\n",
        "print(f\"4. Other hyperparameters: activation = {best_parameters['activation']}, maxiter = 1000\")\n",
        "print(f\"5. Optimization solver: L-BFGS-B\")\n",
        "print(f\"6. Number of iterations: {optimization_result.nit}\")\n",
        "print(f\"7. Optimization time: {optimization_time:.2f} seconds\")\n",
        "print(f\"8. Training Error (MSE, without regularization): {train_mse:.6f}\")\n",
        "print(f\"9. Test Error (MSE): {test_mse:.6f}\")\n",
        "print(f\"10. Initial Training MAPE: {train_mape_init:.4f}%\")\n",
        "print(f\"11. Initial Test MAPE: {test_mape_init:.4f}%\")\n",
        "print(f\"12. Final Training MAPE: {train_mape_final:.4f}%\")\n",
        "print(f\"13. Final Test MAPE: {test_mape_final:.4f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wJsFXLw-u9rx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1675.734443\n",
            "Final loss: 98.689045\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "1. Number of layers L chosen: 4\n",
            "2. Number of neurons N chosen: 64\n",
            "3. Value of lambda chosen: 0.01\n",
            "4. Other hyperparameters: activation = sigmoid, maxiter = 1000\n",
            "5. Optimization solver: L-BFGS-B\n",
            "6. Number of iterations: 1000\n",
            "7. Optimization time: 491.96 seconds\n",
            "8. Training Error (without regularization): 95.003767\n",
            "9. Test Error: 91.734070\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "np.random.seed(42)\n",
        "\n",
        "# Train final model with best parameters\n",
        "start_time = time.time()\n",
        "final_weights, final_biases, optimization_result, initial_loss, final_loss = train_network(\n",
        "    X_train, y_train,\n",
        "    layer_sizes=best_parameters['layers'],\n",
        "    activation=best_parameters['activation'],\n",
        "    lambda_reg=best_parameters['lambda_reg'],\n",
        "    method='L-BFGS-B',\n",
        "    maxiter=1000,\n",
        "    seed=42\n",
        ")\n",
        "# Compute time difference from start\n",
        "end_time = time.time()\n",
        "optimization_time = end_time - start_time\n",
        "\n",
        "# Make predictions on all datasets\n",
        "y_train_pred = predict(X_train, final_weights, final_biases, best_parameters['activation'])\n",
        "y_test_pred = predict(X_test, final_weights, final_biases, best_parameters['activation'])\n",
        "\n",
        "# Compute Mean Squared Error\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "# Compute Mean Absolute Percentage Error\n",
        "train_mape = MAPE(y_train, y_train_pred)\n",
        "test_mape = MAPE(y_test, y_test_pred)\n",
        "\n",
        "# Output results\n",
        "print(f\"1. Number of layers L chosen: {len(best_parameters['layers'])-1}\")\n",
        "print(f\"2. Number of neurons N chosen: {max(best_parameters['layers'][1:-1]) if len(best_parameters['layers'])>2 else best_parameters['layers'][1]}\")\n",
        "print(f\"3. Value of lambda chosen: {best_parameters['lambda_reg']}\")\n",
        "print(f\"4. Other hyperparameters: activation = {best_parameters['activation']}, maxiter = 1000\")\n",
        "print(f\"5. Optimization solver: L-BFGS-B\")\n",
        "print(f\"6. Number of iterations: {optimization_result.nit}\")\n",
        "print(f\"7. Optimization time: {optimization_time:.2f} seconds\")\n",
        "print(f\"8. Training Error (without regularization): {train_mse:.6f}\")\n",
        "print(f\"9. Test Error: {test_mse:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI8IfwTUw3SI"
      },
      "source": [
        "### Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O_C8DE9BtrxZ"
      },
      "outputs": [],
      "source": [
        "# Save detailed results\n",
        "results_summary = {\n",
        "    'best_config': best_parameters,\n",
        "    'optimization_result': {\n",
        "        'success': optimization_result.success,\n",
        "        'message': optimization_result.message,\n",
        "        'iterations': optimization_result.nit,\n",
        "        'optimization_time': optimization_time,\n",
        "        'initial_loss': initial_loss,\n",
        "        'final_loss': final_loss\n",
        "    },\n",
        "    'performance_metrics': {\n",
        "        'train_mse': train_mse,\n",
        "        'test_mse': test_mse,\n",
        "        'train_mape': train_mape,\n",
        "        'test_mape': test_mape\n",
        "    },\n",
        "    'hyperparameter_search_results': search_results\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"results_summary.pkl\", \"wb\") as f:\n",
        "    pickle.dump(results_summary,f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'best_config': {'layers': [32, 64, 32, 16, 1],\n",
              "  'activation': 'sigmoid',\n",
              "  'lambda_reg': 0.01},\n",
              " 'optimization_result': {'success': False,\n",
              "  'message': 'STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT',\n",
              "  'iterations': 1000,\n",
              "  'optimization_time': 491.96037006378174,\n",
              "  'initial_loss': np.float64(1675.7344432251869),\n",
              "  'final_loss': np.float64(98.68904478467041)},\n",
              " 'performance_metrics': {'train_mse': 95.00376742942767,\n",
              "  'test_mse': 91.73407037724682,\n",
              "  'train_mape': np.float64(23.317300386830812),\n",
              "  'test_mape': np.float64(22.763386623945472)},\n",
              " 'hyperparameter_search_results': [{'layers': [32, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(104.25221363501898),\n",
              "   'avg_val_mape': np.float64(24.096009594306775),\n",
              "   'avg_train_loss': np.float64(83.18331023458138),\n",
              "   'avg_train_mape': np.float64(21.42685057717213)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(96.63371678567887),\n",
              "   'avg_val_mape': np.float64(23.34537679954015),\n",
              "   'avg_train_loss': np.float64(90.82189337762743),\n",
              "   'avg_train_mape': np.float64(22.531350422695738)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(96.58223783802161),\n",
              "   'avg_val_mape': np.float64(23.53602070018683),\n",
              "   'avg_train_loss': np.float64(96.10706147109249),\n",
              "   'avg_train_mape': np.float64(23.48058728259242)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(97.6917659902789),\n",
              "   'avg_val_mape': np.float64(23.459180217834632),\n",
              "   'avg_train_loss': np.float64(90.52990956262464),\n",
              "   'avg_train_mape': np.float64(22.467810890833988)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(96.18676140652542),\n",
              "   'avg_val_mape': np.float64(23.455111616516223),\n",
              "   'avg_train_loss': np.float64(95.40510640442783),\n",
              "   'avg_train_mape': np.float64(23.347527434456186)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(98.9352796795102),\n",
              "   'avg_val_mape': np.float64(23.933908572044285),\n",
              "   'avg_train_loss': np.float64(98.7482258882075),\n",
              "   'avg_train_mape': np.float64(23.91041262753355)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(108.96553591433492),\n",
              "   'avg_val_mape': np.float64(24.556252819252606),\n",
              "   'avg_train_loss': np.float64(78.89725738854015),\n",
              "   'avg_train_mape': np.float64(20.77846459505332)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(96.46867116729663),\n",
              "   'avg_val_mape': np.float64(23.31290749555275),\n",
              "   'avg_train_loss': np.float64(91.21878695091925),\n",
              "   'avg_train_mape': np.float64(22.571164184397606)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(96.55456854363393),\n",
              "   'avg_val_mape': np.float64(23.512401063125605),\n",
              "   'avg_train_loss': np.float64(96.09010297729223),\n",
              "   'avg_train_mape': np.float64(23.45694643605727)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(97.02431104667615),\n",
              "   'avg_val_mape': np.float64(23.449831090753417),\n",
              "   'avg_train_loss': np.float64(91.01256365956253),\n",
              "   'avg_train_mape': np.float64(22.564259495181318)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(96.20742070875923),\n",
              "   'avg_val_mape': np.float64(23.442317618729245),\n",
              "   'avg_train_loss': np.float64(95.46289346245901),\n",
              "   'avg_train_mape': np.float64(23.336861460370272)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(98.25692992984528),\n",
              "   'avg_val_mape': np.float64(23.83252822755375),\n",
              "   'avg_train_loss': np.float64(98.11368507771772),\n",
              "   'avg_train_mape': np.float64(23.813851914238494)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(98.13242209312428),\n",
              "   'avg_val_mape': np.float64(23.58486489360992),\n",
              "   'avg_train_loss': np.float64(93.09345358287486),\n",
              "   'avg_train_mape': np.float64(23.010649833003722)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(97.69970652852794),\n",
              "   'avg_val_mape': np.float64(23.47960105512404),\n",
              "   'avg_train_loss': np.float64(90.07069682248478),\n",
              "   'avg_train_mape': np.float64(22.41452980664032)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(97.0134553230472),\n",
              "   'avg_val_mape': np.float64(23.639088979774385),\n",
              "   'avg_train_loss': np.float64(96.14088488163681),\n",
              "   'avg_train_mape': np.float64(23.54750853780182)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(97.67490396462388),\n",
              "   'avg_val_mape': np.float64(23.495000866950246),\n",
              "   'avg_train_loss': np.float64(90.99870465478723),\n",
              "   'avg_train_mape': np.float64(22.611201787670826)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(95.86576076844172),\n",
              "   'avg_val_mape': np.float64(23.42595854311254),\n",
              "   'avg_train_loss': np.float64(94.67539953841066),\n",
              "   'avg_train_mape': np.float64(23.27531656633991)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(100.76620304667965),\n",
              "   'avg_val_mape': np.float64(24.21388533756073),\n",
              "   'avg_train_loss': np.float64(100.52167684957591),\n",
              "   'avg_train_mape': np.float64(24.185302041226095)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(102.98276003098175),\n",
              "   'avg_val_mape': np.float64(23.894047593756074),\n",
              "   'avg_train_loss': np.float64(85.09774202386312),\n",
              "   'avg_train_mape': np.float64(21.768072301114398)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(98.25591873070077),\n",
              "   'avg_val_mape': np.float64(23.467830571355623),\n",
              "   'avg_train_loss': np.float64(89.7157543690754),\n",
              "   'avg_train_mape': np.float64(22.358682010720003)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(96.1965161959594),\n",
              "   'avg_val_mape': np.float64(23.486984841899964),\n",
              "   'avg_train_loss': np.float64(95.42496633422701),\n",
              "   'avg_train_mape': np.float64(23.389985060681454)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(97.10198518082184),\n",
              "   'avg_val_mape': np.float64(23.378074788852782),\n",
              "   'avg_train_loss': np.float64(91.04227620760913),\n",
              "   'avg_train_mape': np.float64(22.60616665966776)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(95.87795388246002),\n",
              "   'avg_val_mape': np.float64(23.40194578955514),\n",
              "   'avg_train_loss': np.float64(94.76427430269929),\n",
              "   'avg_train_mape': np.float64(23.249425731681242)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(99.28243233975083),\n",
              "   'avg_val_mape': np.float64(23.983686305923168),\n",
              "   'avg_train_loss': np.float64(99.1206941533324),\n",
              "   'avg_train_mape': np.float64(23.961840963751126)}]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_summary"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
