{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbUOx2UYNtf1"
      },
      "source": [
        "# Optimization Methods For Data Science\n",
        "## Final Project - Part 2: SVM\n",
        "\n",
        "Géraldine V. Maurer, Viktoriia Vlasenko"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from functions_1j_maurer_vlasenko import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvPqrN1ht4EO"
      },
      "source": [
        "### Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "yZX0PhP1t3jH",
        "outputId": "f0b31afb-a1df-4766-e019-0cfeaf5701c3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feat_1</th>\n",
              "      <th>feat_2</th>\n",
              "      <th>feat_3</th>\n",
              "      <th>feat_4</th>\n",
              "      <th>feat_5</th>\n",
              "      <th>feat_6</th>\n",
              "      <th>feat_7</th>\n",
              "      <th>feat_8</th>\n",
              "      <th>feat_9</th>\n",
              "      <th>feat_10</th>\n",
              "      <th>...</th>\n",
              "      <th>feat_24</th>\n",
              "      <th>feat_25</th>\n",
              "      <th>feat_26</th>\n",
              "      <th>feat_27</th>\n",
              "      <th>feat_28</th>\n",
              "      <th>feat_29</th>\n",
              "      <th>feat_30</th>\n",
              "      <th>feat_31</th>\n",
              "      <th>feat_32</th>\n",
              "      <th>gt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.686191</td>\n",
              "      <td>-0.989465</td>\n",
              "      <td>-0.920503</td>\n",
              "      <td>1.607427</td>\n",
              "      <td>-0.896248</td>\n",
              "      <td>1.118974</td>\n",
              "      <td>-0.969456</td>\n",
              "      <td>1.811707</td>\n",
              "      <td>2.560955</td>\n",
              "      <td>3.803463</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.862891</td>\n",
              "      <td>-0.909545</td>\n",
              "      <td>-0.915361</td>\n",
              "      <td>-0.952061</td>\n",
              "      <td>-0.989461</td>\n",
              "      <td>1.911855</td>\n",
              "      <td>1.409705</td>\n",
              "      <td>2.303997</td>\n",
              "      <td>-0.981840</td>\n",
              "      <td>54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.887917</td>\n",
              "      <td>4.915272</td>\n",
              "      <td>-0.939446</td>\n",
              "      <td>-0.343677</td>\n",
              "      <td>-0.964685</td>\n",
              "      <td>-0.478649</td>\n",
              "      <td>4.342395</td>\n",
              "      <td>-0.332870</td>\n",
              "      <td>-0.768041</td>\n",
              "      <td>-0.815375</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.939201</td>\n",
              "      <td>-0.965917</td>\n",
              "      <td>-0.969461</td>\n",
              "      <td>-0.934799</td>\n",
              "      <td>5.304822</td>\n",
              "      <td>0.934790</td>\n",
              "      <td>-0.410701</td>\n",
              "      <td>0.284690</td>\n",
              "      <td>4.919212</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.923215</td>\n",
              "      <td>2.746968</td>\n",
              "      <td>-0.918085</td>\n",
              "      <td>0.047804</td>\n",
              "      <td>-0.908587</td>\n",
              "      <td>-0.451752</td>\n",
              "      <td>2.984481</td>\n",
              "      <td>0.535007</td>\n",
              "      <td>-0.591029</td>\n",
              "      <td>-0.324043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.809726</td>\n",
              "      <td>-0.929934</td>\n",
              "      <td>-0.891814</td>\n",
              "      <td>-0.881796</td>\n",
              "      <td>3.415373</td>\n",
              "      <td>1.044108</td>\n",
              "      <td>-0.442615</td>\n",
              "      <td>0.033648</td>\n",
              "      <td>2.628199</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.268866</td>\n",
              "      <td>-0.408416</td>\n",
              "      <td>-0.935145</td>\n",
              "      <td>0.731800</td>\n",
              "      <td>-0.922438</td>\n",
              "      <td>0.221781</td>\n",
              "      <td>-0.046606</td>\n",
              "      <td>1.149634</td>\n",
              "      <td>0.592136</td>\n",
              "      <td>1.357959</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.834968</td>\n",
              "      <td>-0.937475</td>\n",
              "      <td>-0.917737</td>\n",
              "      <td>-0.929519</td>\n",
              "      <td>-0.226282</td>\n",
              "      <td>1.608048</td>\n",
              "      <td>0.276169</td>\n",
              "      <td>1.246468</td>\n",
              "      <td>-0.363367</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.529231</td>\n",
              "      <td>-0.829957</td>\n",
              "      <td>-0.897425</td>\n",
              "      <td>0.921280</td>\n",
              "      <td>-0.865304</td>\n",
              "      <td>0.331018</td>\n",
              "      <td>-0.644940</td>\n",
              "      <td>1.296097</td>\n",
              "      <td>1.166863</td>\n",
              "      <td>2.036034</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.775411</td>\n",
              "      <td>-0.881967</td>\n",
              "      <td>-0.864018</td>\n",
              "      <td>-0.908001</td>\n",
              "      <td>-0.784495</td>\n",
              "      <td>1.329586</td>\n",
              "      <td>0.547925</td>\n",
              "      <td>1.195395</td>\n",
              "      <td>-0.810089</td>\n",
              "      <td>35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     feat_1    feat_2    feat_3    feat_4    feat_5    feat_6    feat_7  \\\n",
              "0  2.686191 -0.989465 -0.920503  1.607427 -0.896248  1.118974 -0.969456   \n",
              "1 -0.887917  4.915272 -0.939446 -0.343677 -0.964685 -0.478649  4.342395   \n",
              "2 -0.923215  2.746968 -0.918085  0.047804 -0.908587 -0.451752  2.984481   \n",
              "3 -0.268866 -0.408416 -0.935145  0.731800 -0.922438  0.221781 -0.046606   \n",
              "4  0.529231 -0.829957 -0.897425  0.921280 -0.865304  0.331018 -0.644940   \n",
              "\n",
              "     feat_8    feat_9   feat_10  ...   feat_24   feat_25   feat_26   feat_27  \\\n",
              "0  1.811707  2.560955  3.803463  ... -0.862891 -0.909545 -0.915361 -0.952061   \n",
              "1 -0.332870 -0.768041 -0.815375  ... -0.939201 -0.965917 -0.969461 -0.934799   \n",
              "2  0.535007 -0.591029 -0.324043  ... -0.809726 -0.929934 -0.891814 -0.881796   \n",
              "3  1.149634  0.592136  1.357959  ... -0.834968 -0.937475 -0.917737 -0.929519   \n",
              "4  1.296097  1.166863  2.036034  ... -0.775411 -0.881967 -0.864018 -0.908001   \n",
              "\n",
              "    feat_28   feat_29   feat_30   feat_31   feat_32  gt  \n",
              "0 -0.989461  1.911855  1.409705  2.303997 -0.981840  54  \n",
              "1  5.304822  0.934790 -0.410701  0.284690  4.919212  18  \n",
              "2  3.415373  1.044108 -0.442615  0.033648  2.628199  26  \n",
              "3 -0.226282  1.608048  0.276169  1.246468 -0.363367  33  \n",
              "4 -0.784495  1.329586  0.547925  1.195395 -0.810089  35  \n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"https://raw.githubusercontent.com/gmaurer08/Optimization-Final-Project/refs/heads/main/AGE_PREDICTION.csv\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDmGTiXxuBsj"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-lH4rJruU2L",
        "outputId": "12f6dc76-8366-464a-bf30-06df76d89ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features: 32 columns\n",
            "Data shape: (20475, 32)\n",
            "Target range: 10.00-89.00\n",
            "Train set: (16380, 32)\n",
            "Test set: (4095, 32)\n"
          ]
        }
      ],
      "source": [
        "# Separate features and target\n",
        "feature_cols = [col for col in data.columns if col.startswith('feat')]\n",
        "X = data[feature_cols].values\n",
        "y = data['gt'].values\n",
        "\n",
        "print(f\"Features: {len(feature_cols)} columns\")\n",
        "print(f\"Data shape: {X.shape}\")\n",
        "print(f\"Target range: {y.min():.2f}-{y.max():.2f}\")\n",
        "\n",
        "# Split data into train/test sets\n",
        "n_total = len(X)\n",
        "n_train = int(0.8*n_total)    # 80% for training (used with CV inside)\n",
        "# Remaining 20% for testing\n",
        "\n",
        "# Shuffle indices\n",
        "indices = np.random.permutation(n_total)\n",
        "train_idx = indices[:n_train]\n",
        "test_idx = indices[n_train:]\n",
        "\n",
        "X_train, y_train = X[train_idx], y[train_idx]\n",
        "X_test, y_test = X[test_idx], y[test_idx]\n",
        "\n",
        "# Normalize features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Train set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd8YES8Sunh_"
      },
      "source": [
        "### Find the best hyperparameters with Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhoxaNNHum51",
        "outputId": "72dbf347-c18b-4579-9c33-2987f2f56f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting hyperparameter search\n",
            "\n",
            "[1/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1694.755682\n",
            "Final loss: 86.266267\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 83.4771, Val Loss: 104.7391\n",
            "Train MAPE: 21.2973%, Val MAPE: 24.1226%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1658.215185\n",
            "Final loss: 84.745777\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 82.0352, Val Loss: 107.7196\n",
            "Train MAPE: 21.2439%, Val MAPE: 24.3258%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1653.696334\n",
            "Final loss: 86.308981\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 83.6884, Val Loss: 103.3884\n",
            "Train MAPE: 21.5647%, Val MAPE: 24.0292%\n",
            "Results: Val Loss = 105.2824, Val MAPE = 24.1592%\n",
            "\n",
            "[2/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1658.201936\n",
            "Final loss: 94.615557\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.6366, Val Loss: 96.0188\n",
            "Train MAPE: 22.5568%, Val MAPE: 23.1679%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1661.089640\n",
            "Final loss: 93.647754\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.2713, Val Loss: 97.1536\n",
            "Train MAPE: 22.4511%, Val MAPE: 23.2958%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1661.114901\n",
            "Final loss: 93.887382\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.3159, Val Loss: 96.5673\n",
            "Train MAPE: 22.4933%, Val MAPE: 23.5324%\n",
            "Results: Val Loss = 96.5799, Val MAPE = 23.3320%\n",
            "\n",
            "[3/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1685.046384\n",
            "Final loss: 100.067476\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 96.3940, Val Loss: 95.9927\n",
            "Train MAPE: 23.5786%, Val MAPE: 23.3844%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1679.340697\n",
            "Final loss: 99.571031\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.7714, Val Loss: 97.1612\n",
            "Train MAPE: 23.4321%, Val MAPE: 23.4923%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1660.471175\n",
            "Final loss: 99.536567\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 96.1538, Val Loss: 96.5911\n",
            "Train MAPE: 23.4224%, Val MAPE: 23.7240%\n",
            "Results: Val Loss = 96.5817, Val MAPE = 23.5336%\n",
            "\n",
            "[4/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1767.431046\n",
            "Final loss: 92.899829\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.9474, Val Loss: 96.8837\n",
            "Train MAPE: 22.5306%, Val MAPE: 23.3271%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1608.634509\n",
            "Final loss: 92.425032\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.5409, Val Loss: 97.9745\n",
            "Train MAPE: 22.4910%, Val MAPE: 23.4247%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1635.419026\n",
            "Final loss: 92.834857\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.4362, Val Loss: 97.2990\n",
            "Train MAPE: 22.5417%, Val MAPE: 23.6270%\n",
            "Results: Val Loss = 97.3857, Val MAPE = 23.4596%\n",
            "\n",
            "[5/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1590.581243\n",
            "Final loss: 97.977369\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.5253, Val Loss: 95.7474\n",
            "Train MAPE: 23.4227%, Val MAPE: 23.3226%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1679.457761\n",
            "Final loss: 97.374712\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.0483, Val Loss: 96.6684\n",
            "Train MAPE: 23.2796%, Val MAPE: 23.3979%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1671.475098\n",
            "Final loss: 97.732744\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.5775, Val Loss: 96.1372\n",
            "Train MAPE: 23.3263%, Val MAPE: 23.6368%\n",
            "Results: Val Loss = 96.1843, Val MAPE = 23.4525%\n",
            "\n",
            "[6/24] Testing configuration:\n",
            "Layers: [64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1686.076909\n",
            "Final loss: 109.938456\n",
            "Optimization successful: True\n",
            "Number of iterations: 522\n",
            "Train Loss: 99.1565, Val Loss: 98.4761\n",
            "Train MAPE: 24.0236%, Val MAPE: 23.8191%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1694.984373\n",
            "Final loss: 109.430890\n",
            "Optimization successful: True\n",
            "Number of iterations: 482\n",
            "Train Loss: 98.3568, Val Loss: 99.1761\n",
            "Train MAPE: 23.8549%, Val MAPE: 23.8406%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1681.059830\n",
            "Final loss: 109.580200\n",
            "Optimization successful: True\n",
            "Number of iterations: 501\n",
            "Train Loss: 98.6921, Val Loss: 99.1116\n",
            "Train MAPE: 23.8485%, Val MAPE: 24.1375%\n",
            "Results: Val Loss = 98.9213, Val MAPE = 23.9324%\n",
            "\n",
            "[7/24] Testing configuration:\n",
            "Layers: [128, 64] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1681.716236\n",
            "Final loss: 85.359039\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 82.4215, Val Loss: 104.2518\n",
            "Train MAPE: 21.2730%, Val MAPE: 24.2782%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1674.598108\n",
            "Final loss: 81.268371\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 77.2329, Val Loss: 110.3768\n",
            "Train MAPE: 20.7381%, Val MAPE: 24.4087%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1663.848986\n",
            "Final loss: 84.309301\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 80.8167, Val Loss: 104.9247\n",
            "Train MAPE: 21.0339%, Val MAPE: 24.3308%\n",
            "Results: Val Loss = 106.5178, Val MAPE = 24.3392%\n",
            "\n",
            "[8/24] Testing configuration:\n",
            "Layers: [128, 64] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1677.496373\n",
            "Final loss: 94.199723\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.5085, Val Loss: 96.1920\n",
            "Train MAPE: 22.5861%, Val MAPE: 23.1873%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1664.906231\n",
            "Final loss: 93.651374\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.8517, Val Loss: 96.5933\n",
            "Train MAPE: 22.5003%, Val MAPE: 23.2217%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1693.822274\n",
            "Final loss: 93.920544\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.6598, Val Loss: 96.2143\n",
            "Train MAPE: 22.5418%, Val MAPE: 23.4527%\n",
            "Results: Val Loss = 96.3332, Val MAPE = 23.2872%\n",
            "\n",
            "[9/24] Testing configuration:\n",
            "Layers: [128, 64] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1683.278421\n",
            "Final loss: 98.855797\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 96.4273, Val Loss: 95.9658\n",
            "Train MAPE: 23.5616%, Val MAPE: 23.3606%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1674.896986\n",
            "Final loss: 98.213267\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.6645, Val Loss: 97.0296\n",
            "Train MAPE: 23.3983%, Val MAPE: 23.4614%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1671.908026\n",
            "Final loss: 98.510224\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 96.1481, Val Loss: 96.6126\n",
            "Train MAPE: 23.4061%, Val MAPE: 23.7054%\n",
            "Results: Val Loss = 96.5360, Val MAPE = 23.5091%\n",
            "\n",
            "[10/24] Testing configuration:\n",
            "Layers: [128, 64] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1590.871255\n",
            "Final loss: 93.192829\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.3744, Val Loss: 97.2542\n",
            "Train MAPE: 22.5908%, Val MAPE: 23.3888%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1684.856564\n",
            "Final loss: 92.637177\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.7777, Val Loss: 98.6189\n",
            "Train MAPE: 22.5225%, Val MAPE: 23.4363%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1724.975073\n",
            "Final loss: 93.214230\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.0042, Val Loss: 96.7420\n",
            "Train MAPE: 22.6155%, Val MAPE: 23.5351%\n",
            "Results: Val Loss = 97.5384, Val MAPE = 23.4534%\n",
            "\n",
            "[11/24] Testing configuration:\n",
            "Layers: [128, 64] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1776.781904\n",
            "Final loss: 97.586830\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.6683, Val Loss: 95.7447\n",
            "Train MAPE: 23.4199%, Val MAPE: 23.3004%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1666.894454\n",
            "Final loss: 96.977894\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.1936, Val Loss: 96.7366\n",
            "Train MAPE: 23.2884%, Val MAPE: 23.4055%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1669.499876\n",
            "Final loss: 97.321237\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.6749, Val Loss: 96.2538\n",
            "Train MAPE: 23.3278%, Val MAPE: 23.6507%\n",
            "Results: Val Loss = 96.2451, Val MAPE = 23.4522%\n",
            "\n",
            "[12/24] Testing configuration:\n",
            "Layers: [128, 64] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1732.680258\n",
            "Final loss: 106.393808\n",
            "Optimization successful: True\n",
            "Number of iterations: 337\n",
            "Train Loss: 98.5396, Val Loss: 97.6938\n",
            "Train MAPE: 23.9249%, Val MAPE: 23.7160%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1685.038625\n",
            "Final loss: 105.833677\n",
            "Optimization successful: True\n",
            "Number of iterations: 446\n",
            "Train Loss: 97.8221, Val Loss: 98.6351\n",
            "Train MAPE: 23.7710%, Val MAPE: 23.7677%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1635.233543\n",
            "Final loss: 105.969487\n",
            "Optimization successful: True\n",
            "Number of iterations: 439\n",
            "Train Loss: 98.0562, Val Loss: 98.5204\n",
            "Train MAPE: 23.7562%, Val MAPE: 24.0254%\n",
            "Results: Val Loss = 98.2831, Val MAPE = 23.8364%\n",
            "\n",
            "[13/24] Testing configuration:\n",
            "Layers: [64, 32, 16] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1677.257262\n",
            "Final loss: 90.927643\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 89.3915, Val Loss: 99.4088\n",
            "Train MAPE: 22.3006%, Val MAPE: 23.6470%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1658.563175\n",
            "Final loss: 91.616001\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.3522, Val Loss: 100.1240\n",
            "Train MAPE: 22.5359%, Val MAPE: 23.5678%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1667.071589\n",
            "Final loss: 88.816015\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 87.2334, Val Loss: 100.1247\n",
            "Train MAPE: 22.0632%, Val MAPE: 23.8397%\n",
            "Results: Val Loss = 99.8858, Val MAPE = 23.6848%\n",
            "\n",
            "[14/24] Testing configuration:\n",
            "Layers: [64, 32, 16] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1674.082384\n",
            "Final loss: 94.999808\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.5816, Val Loss: 97.2921\n",
            "Train MAPE: 22.6373%, Val MAPE: 23.2961%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1670.649054\n",
            "Final loss: 93.984095\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.3811, Val Loss: 99.2489\n",
            "Train MAPE: 22.4785%, Val MAPE: 23.4959%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1639.175907\n",
            "Final loss: 94.137622\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.1812, Val Loss: 97.3997\n",
            "Train MAPE: 22.4712%, Val MAPE: 23.5235%\n",
            "Results: Val Loss = 97.9803, Val MAPE = 23.4385%\n",
            "\n",
            "[15/24] Testing configuration:\n",
            "Layers: [64, 32, 16] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1689.686289\n",
            "Final loss: 102.365886\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 96.0617, Val Loss: 96.1145\n",
            "Train MAPE: 23.5582%, Val MAPE: 23.4204%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1683.784418\n",
            "Final loss: 103.540719\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.9561, Val Loss: 97.6599\n",
            "Train MAPE: 23.5328%, Val MAPE: 23.6078%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1671.935185\n",
            "Final loss: 102.516804\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 96.1497, Val Loss: 96.6861\n",
            "Train MAPE: 23.4897%, Val MAPE: 23.7896%\n",
            "Results: Val Loss = 96.8202, Val MAPE = 23.6059%\n",
            "\n",
            "[16/24] Testing configuration:\n",
            "Layers: [64, 32, 16] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1667.805606\n",
            "Final loss: 93.882156\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 92.3663, Val Loss: 97.4104\n",
            "Train MAPE: 22.7821%, Val MAPE: 23.2460%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1709.637964\n",
            "Final loss: 92.331973\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.7272, Val Loss: 99.5465\n",
            "Train MAPE: 22.5528%, Val MAPE: 23.5725%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1591.034029\n",
            "Final loss: 92.195846\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.3629, Val Loss: 97.8249\n",
            "Train MAPE: 22.5511%, Val MAPE: 23.6408%\n",
            "Results: Val Loss = 98.2606, Val MAPE = 23.4864%\n",
            "\n",
            "[17/24] Testing configuration:\n",
            "Layers: [64, 32, 16] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1680.372764\n",
            "Final loss: 98.755681\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 94.8181, Val Loss: 95.3915\n",
            "Train MAPE: 23.3443%, Val MAPE: 23.2770%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1676.113059\n",
            "Final loss: 98.187051\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 94.2520, Val Loss: 96.4108\n",
            "Train MAPE: 23.1962%, Val MAPE: 23.3955%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1630.604404\n",
            "Final loss: 98.612901\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 94.9272, Val Loss: 95.5992\n",
            "Train MAPE: 23.2695%, Val MAPE: 23.5644%\n",
            "Results: Val Loss = 95.8005, Val MAPE = 23.4123%\n",
            "\n",
            "[18/24] Testing configuration:\n",
            "Layers: [64, 32, 16] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1706.288588\n",
            "Final loss: 118.337777\n",
            "Optimization successful: True\n",
            "Number of iterations: 396\n",
            "Train Loss: 100.9758, Val Loss: 100.5952\n",
            "Train MAPE: 24.2897%, Val MAPE: 24.1316%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1626.303464\n",
            "Final loss: 117.953092\n",
            "Optimization successful: True\n",
            "Number of iterations: 735\n",
            "Train Loss: 100.0203, Val Loss: 100.6811\n",
            "Train MAPE: 24.1219%, Val MAPE: 24.0697%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1739.929842\n",
            "Final loss: 118.046430\n",
            "Optimization successful: True\n",
            "Number of iterations: 745\n",
            "Train Loss: 100.4949, Val Loss: 100.9665\n",
            "Train MAPE: 24.1344%, Val MAPE: 24.4315%\n",
            "Results: Val Loss = 100.7476, Val MAPE = 24.2109%\n",
            "\n",
            "[19/24] Testing configuration:\n",
            "Layers: [128, 64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1670.699758\n",
            "Final loss: 87.927935\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 86.2731, Val Loss: 103.2879\n",
            "Train MAPE: 21.8649%, Val MAPE: 23.6913%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1671.379964\n",
            "Final loss: 86.496928\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 84.6122, Val Loss: 103.6406\n",
            "Train MAPE: 21.7990%, Val MAPE: 23.9943%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.001\n",
            "Initial loss: 1673.337039\n",
            "Final loss: 80.159127\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 77.1644, Val Loss: 106.8152\n",
            "Train MAPE: 20.5864%, Val MAPE: 24.3904%\n",
            "Results: Val Loss = 104.5812, Val MAPE = 24.0253%\n",
            "\n",
            "[20/24] Testing configuration:\n",
            "Layers: [128, 64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1685.177592\n",
            "Final loss: 93.811366\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.4703, Val Loss: 97.3485\n",
            "Train MAPE: 22.4076%, Val MAPE: 23.2415%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1652.357716\n",
            "Final loss: 93.047765\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 89.6062, Val Loss: 99.4437\n",
            "Train MAPE: 22.3457%, Val MAPE: 23.4675%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.01\n",
            "Initial loss: 1687.032389\n",
            "Final loss: 93.044339\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 89.2291, Val Loss: 97.8197\n",
            "Train MAPE: 22.3889%, Val MAPE: 23.5944%\n",
            "Results: Val Loss = 98.2039, Val MAPE = 23.4344%\n",
            "\n",
            "[21/24] Testing configuration:\n",
            "Layers: [128, 64, 32] (hidden)\n",
            "Activation: tanh\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1713.545970\n",
            "Final loss: 99.664951\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.5216, Val Loss: 95.7859\n",
            "Train MAPE: 23.4466%, Val MAPE: 23.3291%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1681.477845\n",
            "Final loss: 99.042346\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.0325, Val Loss: 96.7385\n",
            "Train MAPE: 23.3231%, Val MAPE: 23.4391%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: tanh, Lambda: 0.1\n",
            "Initial loss: 1684.870605\n",
            "Final loss: 99.448937\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.5721, Val Loss: 96.1189\n",
            "Train MAPE: 23.3521%, Val MAPE: 23.6448%\n",
            "Results: Val Loss = 96.2144, Val MAPE = 23.4710%\n",
            "\n",
            "[22/24] Testing configuration:\n",
            "Layers: [128, 64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.001\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1666.552540\n",
            "Final loss: 93.165113\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.8693, Val Loss: 97.2838\n",
            "Train MAPE: 22.6871%, Val MAPE: 23.3230%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1796.617681\n",
            "Final loss: 92.491131\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 91.0511, Val Loss: 98.7724\n",
            "Train MAPE: 22.5766%, Val MAPE: 23.4579%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.001\n",
            "Initial loss: 1692.637719\n",
            "Final loss: 92.744313\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 90.6969, Val Loss: 97.3718\n",
            "Train MAPE: 22.6251%, Val MAPE: 23.5308%\n",
            "Results: Val Loss = 97.8093, Val MAPE = 23.4372%\n",
            "\n",
            "[23/24] Testing configuration:\n",
            "Layers: [128, 64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.01\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1655.147113\n",
            "Final loss: 97.931999\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 94.7577, Val Loss: 95.5640\n",
            "Train MAPE: 23.2974%, Val MAPE: 23.2618%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1726.600705\n",
            "Final loss: 97.387296\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 94.3229, Val Loss: 96.3902\n",
            "Train MAPE: 23.1835%, Val MAPE: 23.3697%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1581.182558\n",
            "Final loss: 97.865506\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "Train Loss: 95.2421, Val Loss: 95.8307\n",
            "Train MAPE: 23.2797%, Val MAPE: 23.5859%\n",
            "Results: Val Loss = 95.9283, Val MAPE = 23.4058%\n",
            "\n",
            "[24/24] Testing configuration:\n",
            "Layers: [128, 64, 32] (hidden)\n",
            "Activation: sigmoid\n",
            "Lambda: 0.1\n",
            "\n",
            "Starting 3-fold cross-validation...\n",
            "\n",
            "Fold 1/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1664.722402\n",
            "Final loss: 112.148580\n",
            "Optimization successful: True\n",
            "Number of iterations: 319\n",
            "Train Loss: 99.4649, Val Loss: 98.8640\n",
            "Train MAPE: 24.0594%, Val MAPE: 23.8693%\n",
            "\n",
            "Fold 2/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1675.431589\n",
            "Final loss: 111.716439\n",
            "Optimization successful: True\n",
            "Number of iterations: 420\n",
            "Train Loss: 98.8369, Val Loss: 99.5065\n",
            "Train MAPE: 23.9216%, Val MAPE: 23.8902%\n",
            "\n",
            "Fold 3/3\n",
            "Training network with architecture: [32, 128, 64, 32, 1]\n",
            "Activation: sigmoid, Lambda: 0.1\n",
            "Initial loss: 1621.823362\n",
            "Final loss: 111.820819\n",
            "Optimization successful: True\n",
            "Number of iterations: 345\n",
            "Train Loss: 99.0594, Val Loss: 99.4762\n",
            "Train MAPE: 23.9038%, Val MAPE: 24.1909%\n",
            "Results: Val Loss = 99.2823, Val MAPE = 23.9834%\n",
            "\n",
            "\n",
            "HYPERPARAMETER SEARCH COMPLETE\n",
            "Best validation loss: 95.800493\n",
            "Best configuration:\n",
            "Hidden layers: [64, 32, 16]\n",
            "Activations: sigmoid\n",
            "Lambda: 0.01\n"
          ]
        }
      ],
      "source": [
        "# Find best hyperparameters using cross-validation\n",
        "best_parameters, search_results = hyperparameter_search(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-8c4C5duvAB"
      },
      "source": [
        "### Training the final model with the best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wJsFXLw-u9rx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training network with architecture: [32, 64, 32, 16, 1]\n",
            "Activation: sigmoid, Lambda: 0.01\n",
            "Initial loss: 1756.619372\n",
            "Final loss: 98.697318\n",
            "Optimization successful: False\n",
            "Number of iterations: 1000\n",
            "\n",
            "Optimization details:\n",
            "Success: False\n",
            "Message: STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
            "Iterations: 1000\n",
            "Loss reduction: 1756.619372 -> 98.697318\n"
          ]
        }
      ],
      "source": [
        "# Train final model with best parameters\n",
        "final_weights, final_biases, optimization_result, initial_loss, final_loss = train_network(\n",
        "    X_train, y_train,\n",
        "    layer_sizes=best_parameters['layers'],\n",
        "    activation=best_parameters['activation'],\n",
        "    lambda_reg=best_parameters['lambda_reg'],\n",
        "    method='L-BFGS-B',\n",
        "    maxiter=1000\n",
        ")\n",
        "\n",
        "print(f\"\\nOptimization details:\")\n",
        "print(f\"Success: {optimization_result.success}\")\n",
        "print(f\"Message: {optimization_result.message}\")\n",
        "print(f\"Iterations: {optimization_result.nit}\")\n",
        "print(f\"Loss reduction: {initial_loss:.6f} -> {final_loss:.6f}\")\n",
        "\n",
        "# Make predictions on all datasets\n",
        "y_train_pred = predict(X_train, final_weights, final_biases, best_parameters['activation'])\n",
        "y_test_pred = predict(X_test, final_weights, final_biases, best_parameters['activation'])\n",
        "\n",
        "# Compute Mean Squared Error\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "# Compute Mean Absolute Percentage Error\n",
        "train_mape = MAPE(y_train, y_train_pred)\n",
        "test_mape = MAPE(y_test, y_test_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI8IfwTUw3SI"
      },
      "source": [
        "### Print Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_C8DE9BtrxZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best Network Configuration:\n",
            "Architecture: [32, 64, 32, 16, 1]\n",
            "Hidden layers: [64, 32, 16]\n",
            "Activation function: sigmoid\n",
            "Regularization (lambda): 0.01\n",
            "Total parameters: 4737\n",
            "\n",
            "Optimization Results:\n",
            "Method: L-BFGS-B\n",
            "Max iterations: 1000\n",
            "Convergence: Failed\n",
            "Iterations used: 1000\n",
            "Initial loss: 1756.619372\n",
            "Final loss: 98.697318\n",
            "Loss reduction: 94.38%\n",
            "\n",
            "Performance Metrics:\n",
            "Training MSE: 94.948496\n",
            "Test MSE: 91.639851\n",
            "Training MAPE: 23.3081%\n",
            "Test MAPE: 22.7537%\n",
            "\n",
            "Model Insights:\n",
            "Best performing activation: sigmoid\n",
            "Optimal regularization strength: 0.01\n",
            "Network depth: 4 layers\n",
            "Network width: 64 neurons (max)\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\nBest Network Configuration:\")\n",
        "print(f\"Architecture: {best_parameters['layers']}\")\n",
        "print(f\"Hidden layers: {best_parameters['layers'][1:-1]}\")\n",
        "print(f\"Activation function: {best_parameters['activation']}\")\n",
        "print(f\"Regularization (lambda): {best_parameters['lambda_reg']}\")\n",
        "print(f\"Total parameters: {sum(w.size + b.size for w, b in zip(final_weights, final_biases))}\")\n",
        "\n",
        "print(f\"\\nOptimization Results:\")\n",
        "print(f\"Method: L-BFGS-B\")\n",
        "print(f\"Max iterations: 1000\")\n",
        "print(f\"Convergence: {'Successful' if optimization_result.success else 'Failed'}\")\n",
        "print(f\"Iterations used: {optimization_result.nit}\")\n",
        "print(f\"Initial loss: {initial_loss:.6f}\")\n",
        "print(f\"Final loss: {final_loss:.6f}\")\n",
        "print(f\"Loss reduction: {((initial_loss-final_loss) / initial_loss*100):.2f}%\")\n",
        "\n",
        "print(f\"\\nPerformance Metrics:\")\n",
        "print(f\"Training MSE: {train_mse:.6f}\")\n",
        "print(f\"Test MSE: {test_mse:.6f}\")\n",
        "print(f\"Training MAPE: {train_mape:.4f}%\")\n",
        "print(f\"Test MAPE: {test_mape:.4f}%\")\n",
        "\n",
        "print(f\"\\nModel Insights:\")\n",
        "print(f\"Best performing activation: {best_parameters['activation']}\")\n",
        "print(f\"Optimal regularization strength: {best_parameters['lambda_reg']}\")\n",
        "print(f\"Network depth: {len(best_parameters['layers'])-1} layers\")\n",
        "print(f\"Network width: {max(best_parameters['layers'][1:-1])} neurons (max)\")\n",
        "\n",
        "\n",
        "# Save detailed results\n",
        "results_summary = {\n",
        "    'best_config': best_parameters,\n",
        "    'optimization_result': {\n",
        "        'success': optimization_result.success,\n",
        "        'message': optimization_result.message,\n",
        "        'iterations': optimization_result.nit,\n",
        "        'initial_loss': initial_loss,\n",
        "        'final_loss': final_loss\n",
        "    },\n",
        "    'performance_metrics': {\n",
        "        'train_mse': train_mse,\n",
        "        'test_mse': test_mse,\n",
        "        'train_mape': train_mape,\n",
        "        'test_mape': test_mape\n",
        "    },\n",
        "    'hyperparameter_search_results': search_results\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"results_summary.pkl\", \"wb\") as f:\n",
        "    pickle.dump(results_summary,f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'best_config': {'layers': [32, 64, 32, 16, 1],\n",
              "  'activation': 'sigmoid',\n",
              "  'lambda_reg': 0.01},\n",
              " 'optimization_result': {'success': False,\n",
              "  'message': 'STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT',\n",
              "  'iterations': 1000,\n",
              "  'initial_loss': np.float64(1756.6193722644898),\n",
              "  'final_loss': np.float64(98.69731751643677)},\n",
              " 'performance_metrics': {'train_mse': 94.94849595886431,\n",
              "  'test_mse': 91.63985098915778,\n",
              "  'train_mape': np.float64(23.308067409477044),\n",
              "  'test_mape': np.float64(22.753721138685908)},\n",
              " 'hyperparameter_search_results': [{'layers': [32, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(105.28236443534229),\n",
              "   'avg_val_mape': np.float64(24.15918092646488),\n",
              "   'avg_train_loss': np.float64(83.06689650781153),\n",
              "   'avg_train_mape': np.float64(21.368623353417735)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(96.57989694851581),\n",
              "   'avg_val_mape': np.float64(23.33201616016622),\n",
              "   'avg_train_loss': np.float64(90.74128528608217),\n",
              "   'avg_train_mape': np.float64(22.50038821875548)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(96.58168979669226),\n",
              "   'avg_val_mape': np.float64(23.533563156889347),\n",
              "   'avg_train_loss': np.float64(96.1063984376835),\n",
              "   'avg_train_mape': np.float64(23.477681609121948)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(97.38572320007745),\n",
              "   'avg_val_mape': np.float64(23.45958053242103),\n",
              "   'avg_train_loss': np.float64(90.6414750373043),\n",
              "   'avg_train_mape': np.float64(22.5210897778736)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(96.1843494077687),\n",
              "   'avg_val_mape': np.float64(23.4524690560708),\n",
              "   'avg_train_loss': np.float64(95.38371065398205),\n",
              "   'avg_train_mape': np.float64(23.34285539577247)},\n",
              "  {'layers': [32, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(98.92126178941129),\n",
              "   'avg_val_mape': np.float64(23.932410492009257),\n",
              "   'avg_train_loss': np.float64(98.73512858360311),\n",
              "   'avg_train_mape': np.float64(23.908984126136172)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(106.51779187794801),\n",
              "   'avg_val_mape': np.float64(24.339220692569214),\n",
              "   'avg_train_loss': np.float64(80.15701376218526),\n",
              "   'avg_train_mape': np.float64(21.01499127303705)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(96.33316305106173),\n",
              "   'avg_val_mape': np.float64(23.287229768754944),\n",
              "   'avg_train_loss': np.float64(91.00663977662225),\n",
              "   'avg_train_mape': np.float64(22.542755218719225)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(96.53600575058981),\n",
              "   'avg_val_mape': np.float64(23.509141889937798),\n",
              "   'avg_train_loss': np.float64(96.07995643462631),\n",
              "   'avg_train_mape': np.float64(23.455354614217715)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(97.53837173472267),\n",
              "   'avg_val_mape': np.float64(23.453389105952578),\n",
              "   'avg_train_loss': np.float64(91.0520957078284),\n",
              "   'avg_train_mape': np.float64(22.576282087931265)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(96.245066062104),\n",
              "   'avg_val_mape': np.float64(23.452182378086068),\n",
              "   'avg_train_loss': np.float64(95.51230883514854),\n",
              "   'avg_train_mape': np.float64(23.345366073351986)},\n",
              "  {'layers': [32, 128, 64, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(98.28308862448652),\n",
              "   'avg_val_mape': np.float64(23.836367601883165),\n",
              "   'avg_train_loss': np.float64(98.13929218882923),\n",
              "   'avg_train_mape': np.float64(23.81737693711503)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(99.88581065623845),\n",
              "   'avg_val_mape': np.float64(23.684823314452604),\n",
              "   'avg_train_loss': np.float64(88.99235548867625),\n",
              "   'avg_train_mape': np.float64(22.299907950542817)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(97.9802640698869),\n",
              "   'avg_val_mape': np.float64(23.438514278710922),\n",
              "   'avg_train_loss': np.float64(90.71464792550609),\n",
              "   'avg_train_mape': np.float64(22.528983317601206)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(96.82016172616244),\n",
              "   'avg_val_mape': np.float64(23.605911620149588),\n",
              "   'avg_train_loss': np.float64(96.05586905341642),\n",
              "   'avg_train_mape': np.float64(23.52689851052132)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(98.26061803195962),\n",
              "   'avg_val_mape': np.float64(23.486416898489036),\n",
              "   'avg_train_loss': np.float64(91.1521467852981),\n",
              "   'avg_train_mape': np.float64(22.62867995865354)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(95.80049262143079),\n",
              "   'avg_val_mape': np.float64(23.412311798280673),\n",
              "   'avg_train_loss': np.float64(94.6657768043825),\n",
              "   'avg_train_mape': np.float64(23.270015585685982)},\n",
              "  {'layers': [32, 64, 32, 16, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(100.7476180379781),\n",
              "   'avg_val_mape': np.float64(24.210947663133197),\n",
              "   'avg_train_loss': np.float64(100.49697653574572),\n",
              "   'avg_train_mape': np.float64(24.181985146027824)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(104.58120277864703),\n",
              "   'avg_val_mape': np.float64(24.025344173900248),\n",
              "   'avg_train_loss': np.float64(82.68322342887348),\n",
              "   'avg_train_mape': np.float64(21.416769670674768)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(98.20394111813677),\n",
              "   'avg_val_mape': np.float64(23.434442010834683),\n",
              "   'avg_train_loss': np.float64(89.76853831904674),\n",
              "   'avg_train_mape': np.float64(22.38075227256826)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'tanh',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(96.21444648067124),\n",
              "   'avg_val_mape': np.float64(23.470996146800584),\n",
              "   'avg_train_loss': np.float64(95.37536618750691),\n",
              "   'avg_train_mape': np.float64(23.37393034615555)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.001,\n",
              "   'avg_val_loss': np.float64(97.80930675525467),\n",
              "   'avg_val_mape': np.float64(23.437222606015695),\n",
              "   'avg_train_loss': np.float64(91.20579894304558),\n",
              "   'avg_train_mape': np.float64(22.629598016181912)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.01,\n",
              "   'avg_val_loss': np.float64(95.92826975579833),\n",
              "   'avg_val_mape': np.float64(23.40580737049832),\n",
              "   'avg_train_loss': np.float64(94.7742109552101),\n",
              "   'avg_train_mape': np.float64(23.25352962762741)},\n",
              "  {'layers': [32, 128, 64, 32, 1],\n",
              "   'activation': 'sigmoid',\n",
              "   'lambda_reg': 0.1,\n",
              "   'avg_val_loss': np.float64(99.28225397452665),\n",
              "   'avg_val_mape': np.float64(23.98344539408994),\n",
              "   'avg_train_loss': np.float64(99.12037307499533),\n",
              "   'avg_train_mape': np.float64(23.961601614062587)}]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_summary"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
